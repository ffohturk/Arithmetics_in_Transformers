{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "from requests import get\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDataset_pp(ndig, nextra):\n",
    "\n",
    "    stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11, 'x': 12}\n",
    "\n",
    "    data = []\n",
    "    target = []\n",
    "    data_f = []\n",
    "    target_f = []\n",
    "\n",
    "    k = 0\n",
    "    while k < 200000:\n",
    "        i = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "        j = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        if len(li) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(li), size=(1,))\n",
    "            r1 = ndig+nextra - len(li) - r0\n",
    "            li = ['x'] * r0 + li + ['x'] * r1\n",
    "        if len(lj) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(lj), size=(1,))\n",
    "            r1 = ndig+nextra - len(lj) - r0\n",
    "            lj = ['x'] * r0 + lj + ['x'] * r1\n",
    "        if len(lij) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(lij), size=(1,))\n",
    "            r1 = ndig+nextra - len(lij) - r0\n",
    "            if r1 > 0:\n",
    "                lij = ['x'] * r0 + lij + ['='] + ['x'] * (r1 - 1)\n",
    "            else:\n",
    "                lij = ['x'] * r0 + lij\n",
    "\n",
    "        lsum = li + ['+'] + lj + lij\n",
    "\n",
    "        if lsum[-1] in ['=', 'x']:\n",
    "            lt = lsum[1:] + ['x']\n",
    "        else:\n",
    "            lt = lsum[1:] + ['=']\n",
    "        data.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "\n",
    "        include = False\n",
    "        while not include:\n",
    "            i = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            j = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            include = (i + j < 10**(ndig+nextra))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        if len(li) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(li), size=(1,))\n",
    "            r1 = ndig+nextra - len(li) - r0\n",
    "            li = ['x'] * r0 + li + ['x'] * r1\n",
    "        if len(lj) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(lj), size=(1,))\n",
    "            r1 = ndig+nextra - len(lj) - r0\n",
    "            lj = ['x'] * r0 + lj + ['x'] * r1\n",
    "        if len(lij) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(lij), size=(1,))\n",
    "            r1 = ndig+nextra - len(lij) - r0\n",
    "            if r1 > 0:\n",
    "                lij = ['x'] * r0 + lij + ['='] + ['x'] * (r1 - 1)\n",
    "            else:\n",
    "                lij = ['x'] * r0 + lij\n",
    "\n",
    "        lsum = li + ['+'] + lj + lij\n",
    "\n",
    "        if lsum[-1] in ['=', 'x']:\n",
    "            lt = lsum[1:] + ['x']\n",
    "        else:\n",
    "            lt = lsum[1:] + ['=']\n",
    "\n",
    "        data_f.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target_f.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "        k += 1\n",
    "\n",
    "    data_f = torch.LongTensor(data_f)\n",
    "    target_f = torch.LongTensor(target_f)\n",
    "    data = torch.LongTensor(data)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    vocab = len(stoi)\n",
    "    \n",
    "    return vocab, data, target, data_f, target_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTot(nn.Module):\n",
    "\n",
    "  def __init__(self, decoder, embed, generator):\n",
    "    super().__init__()\n",
    "    self.embed = embed\n",
    "    self.gen = generator\n",
    "    self.decoder = decoder\n",
    "    self.generator = generator\n",
    "\n",
    "  def forward(self, src, mask):\n",
    "    return self.generator(self.decoder(self.embed(src), mask))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, vocab_size):\n",
    "    super().__init__()\n",
    "    self.ln = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return F.log_softmax(self.ln(x), dim=-1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, attn, ffn, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        x1 = self.norm(x)\n",
    "        x = x + self.dropout(self.attn(x1, x1, x1, mask))\n",
    "        self.out = x + self.dropout(self.ffn(self.norm(x)))\n",
    "        return self.out\n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.norm = nn.LayerNorm(layer.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "def Attention(q, k, v, theta, mask=None, dropout=None):\n",
    "\n",
    "            ### -- Softmax Attention with RoFormer -- ###\n",
    "\n",
    "            # q, k, v are dims (batch_size, # heads, seq_len, d_{k,v})\n",
    "\n",
    "            m = torch.arange(k.shape[-2]).view(k.shape[-2], 1).to(q.device)\n",
    "            t = torch.arange(k.shape[-1]).view(1, k.shape[-1])\n",
    "            t = torch.exp( - ( 2 * np.log(theta) / k.shape[-1] ) * torch.floor(t/2.) ).to(q.device)\n",
    "            r1 = torch.cos(m * t)\n",
    "            r2 = torch.sin(m * t)\n",
    "\n",
    "            K = torch.cat((q, k, v))\n",
    "\n",
    "            Kp = torch.einsum('ijkl, kl -> ijkl', K, r1)\n",
    "\n",
    "            L = torch.kron(torch.eye(k.shape[-1]//2), torch.Tensor([[0,-1],[1,0]])).to(q.device)\n",
    "            K = torch.einsum('ijkl, ml -> ijkm', K, L)\n",
    "\n",
    "            Kp += torch.einsum('ijkl, kl -> ijkl', K, r2)\n",
    "\n",
    "            Kp = Kp.view(-1, k.shape[0], k.shape[1], k.shape[2], k.shape[-1])\n",
    "\n",
    "            q, k, v = Kp[0], Kp[1], v # Kp[2]\n",
    "\n",
    "            A = torch.matmul(q, k.transpose(-2,-1)) * k.size(-1)**(-0.5)\n",
    "\n",
    "            if mask is not None:\n",
    "                A.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "            O = F.softmax(A, dim=-1)\n",
    "\n",
    "            if dropout is not None:\n",
    "                O = dropout(O)\n",
    "\n",
    "            return torch.matmul(O, v), O\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.attn = None\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.dropout =  nn.Dropout(p=dropout)\n",
    "        self.theta = 1000.0\n",
    "\n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        x = [l(z).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) for l, z in zip(self.linears, (query, keys, values))]\n",
    "\n",
    "        y, self.attn = Attention(x[0], x[1], x[2], self.theta, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linears[-1](y)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = None\n",
    "        self.out_p = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = self.relu(self.w1(x))\n",
    "        self.out_p = self.w2(self.dropout(self.out))\n",
    "        return self.w2(self.dropout(self.out))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.Emb = nn.Embedding(src_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Emb(x) * np.sqrt(self.d_model)\n",
    "\n",
    "def make_model(vocab, N = 6, d_model = 512, d_ff = 2048, h = 8, dropout = 0.1):\n",
    "\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    model = DecoderTot(DecoderStack(Decoder( c(attn), c(ffn), d_model, dropout), N),\n",
    "                           Embeddings(vocab, d_model),  Generator(d_model, vocab))\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1: # This is there to not initialize the biases\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    # print('# of parameters =', sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "\n",
    "    return model\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, target):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = self.inputs[index]\n",
    "        tgt = self.target[index]\n",
    "\n",
    "        return src, tgt\n",
    "\n",
    "def prepare(rank, world_size, data, target, batch_size, pin_memory=True, num_workers=0):\n",
    "\n",
    "    dataset = Dataset(data, target)\n",
    "    # sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "\n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def accuracy_calc(inputs, targets, max_gen, ignore_idx):\n",
    "    acc = []\n",
    "    for i in range(inputs.shape[0]):\n",
    "        in_z = inputs[i, -max_gen:][inputs[i, -max_gen:] != ignore_idx]\n",
    "        target_z = targets[i, -max_gen:][targets[i, -max_gen:] != ignore_idx]\n",
    "        try:\n",
    "            acc_z = (in_z == target_z).float().min().item()\n",
    "\n",
    "            acc.append(acc_z)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(acc) == 0:\n",
    "        return 'zero', 0.0\n",
    "    \n",
    "    return 'non-zero', sum(acc) / len(acc)\n",
    "\n",
    "def run_epoch(data, loader, model, optimizer, device, status='train'):\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        src, tgt = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        ignore_idx = 12\n",
    "\n",
    "        seq_len = src.shape[-1]\n",
    "        num_digits = (seq_len - 1) // 3\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
    "        logits = model.forward(src, mask)[:, -(num_digits + 1):]\n",
    "        tgt = tgt[:, -(num_digits + 1):]\n",
    "\n",
    "        kl_loss = nn.CrossEntropyLoss(ignore_index=ignore_idx)\n",
    "\n",
    "        loss = kl_loss(logits.transpose(-1, -2), tgt) # We want inputs to be (bs, vocab_size, seq len), so needed a transpose. Targets are (bs, seq len) with values in [0, vocab_size]\n",
    "\n",
    "        l = loss.detach().item()\n",
    "\n",
    "        if status == 'train':\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pre_data = torch.tensor(l) # correctness, loss\n",
    "            if i % 200 == 0:\n",
    "                print(f'---{status} loss ---')\n",
    "                print(l)\n",
    "\n",
    "        if status == 'eval':\n",
    "            # s, a = accuracy_calc(torch.argmax(logits.detach(), dim=-1), tgt, num_digits + 1, ignore_idx=ignore_idx)\n",
    "            w2 = sum((p.data**2).sum() for p in model.parameters()).clone().detach().to('cpu')\n",
    "            pre_data = torch.tensor([l, w2.item()])\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                print(f'---{status} loss ---')\n",
    "                print(l)\n",
    "                # print(s, a)\n",
    "        \n",
    "        data = torch.cat((data, pre_data.unsqueeze(0)), 0)\n",
    "\n",
    "        del loss, tgt, src, logits\n",
    "        gc.collect\n",
    "\n",
    "    return data\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    ### Dataset\n",
    "\n",
    "    print('--- Generating data ---')\n",
    "    vocab, data, target, data_f, target_f = GenerateDataset_pp(ndig=args.ndigits, nextra = args.nextra)\n",
    "    print('--- Finished generating data ---')\n",
    "    # Three way split (training, test)\n",
    "    random.seed()\n",
    "    z = list(zip(data.tolist(), target.tolist()))\n",
    "    random.shuffle(z)\n",
    "\n",
    "    z1, z2 = zip(*z)\n",
    "    src_array_sh, tgt_array_sh = torch.LongTensor(list(z1)), torch.LongTensor(list(z2))\n",
    "\n",
    "    rank = torch.device('cuda:0')\n",
    "    world_size = 0\n",
    "\n",
    "    # Dataset parameters\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    batch_size_eval = args.batch_size_eval\n",
    "\n",
    "    vocab = vocab\n",
    "\n",
    "    split = args.split\n",
    "\n",
    "    n1 = int(split*len(src_array_sh))\n",
    "    n2 = 2*n1\n",
    "    # n1 = 0\n",
    "    # n0 = 100\n",
    "    priming_examples = 0\n",
    "\n",
    "    src_train, src_test = src_array_sh[:n1], src_array_sh[n1:n2]\n",
    "    tgt_train, tgt_test = tgt_array_sh[:n1], tgt_array_sh[n1:n2]\n",
    "    src_long, tgt_long = data_f[:priming_examples], target_f[:priming_examples]\n",
    "    src_test_long, tgt_test_long = data_f[priming_examples:], target_f[priming_examples:]\n",
    "\n",
    "    # src_train = torch.cat((src_train, src_long), 0)\n",
    "    # tgt_train = torch.cat((tgt_train, tgt_long), 0)\n",
    "    # src_train = src_long\n",
    "    # tgt_train = tgt_long\n",
    "\n",
    "    random.seed()\n",
    "    z = list(zip(src_train.tolist(), tgt_train.tolist()))\n",
    "    random.shuffle(z)\n",
    "\n",
    "    z1, z2 = zip(*z)\n",
    "    src_train, tgt_train = torch.LongTensor(list(z1)), torch.LongTensor(list(z2))\n",
    "\n",
    "    dataloader_train = prepare(rank, world_size, src_train, tgt_train, batch_size)\n",
    "\n",
    "    dataloader_test = prepare(rank, world_size, src_test, tgt_test, batch_size_eval)\n",
    "\n",
    "    dataloader_test_long = prepare(rank, world_size, src_test_long, tgt_test_long, batch_size_eval)\n",
    "\n",
    "    ### Model\n",
    "\n",
    "    d_model = args.d_model\n",
    "    d_ff = args.d_ff\n",
    "    n_heads = args.heads\n",
    "    n_layers = args.num_layers\n",
    "\n",
    "    # model = make_model(d_model=d_model,\n",
    "    #                    n_heads=n_heads,\n",
    "    #                    d_ff=d_ff,\n",
    "    #                    vocab=vocab,\n",
    "    #                    N=n_layers\n",
    "    #                    )\n",
    "\n",
    "    model = make_model(vocab, N = n_layers, d_model = d_model, d_ff = d_ff, h = n_heads, dropout = 0.1)\n",
    "\n",
    "    model = model.to(rank)\n",
    "\n",
    "    ### Training parameters and optimizer\n",
    "\n",
    "    lr = args.learning_rate\n",
    "    weight_decay = args.weight_decay    \n",
    "    num_epochs = args.epochs\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr = lr,\n",
    "                                  betas = (0.9, 0.98),\n",
    "                                  eps=1e-8,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "    ### Tracking\n",
    "\n",
    "    theta = model.decoder.layers[0].attn.theta\n",
    "\n",
    "    wandb.init(project=\"Generative Addition\",\n",
    "                     config={\"lr\": lr,\n",
    "                             \"split\":split,\n",
    "                             \"layers\": n_layers,\n",
    "                             \"weight decay\": weight_decay,\n",
    "                             \"d_ff\": d_ff,\n",
    "                             \"d_model\": d_model,\n",
    "                             \"heads\": n_heads,\n",
    "                             \"theta\": theta,\n",
    "                             \"Priming examples\": priming_examples,\n",
    "                             \"n digits train\": args.ndigits,\n",
    "                             \"n digits test\": f'{args.ndigits} + {args.nextra}',\n",
    "                             \"batch size\": batch_size,\n",
    "                             \"batch_size_eval\": batch_size_eval,\n",
    "                             'random padding': True,\n",
    "                             'ID': args.ID,\n",
    "                             }\n",
    "                        )\n",
    "\n",
    "    ### Training    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f' --- {epoch} ---')\n",
    "        data = torch.tensor([])\n",
    "        data_t = torch.tensor([])\n",
    "        data_tl = torch.tensor([])\n",
    "\n",
    "        model.train()\n",
    "        data = run_epoch(data, loader=dataloader_train, model=model, optimizer=optimizer, device=rank, status='train')\n",
    "        \n",
    "        data = data.mean(dim=0)\n",
    "        s = {}\n",
    "        s['training loss'] = data\n",
    "        # s['training acc.'] = data[-2]\n",
    "        wandb.log(s,\n",
    "            step=epoch\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_t = run_epoch(data_t, loader=dataloader_test, model=model, optimizer=optimizer, device=rank, status='eval')\n",
    "\n",
    "            data_tl = run_epoch(data_tl, loader=dataloader_test_long, model=model, optimizer=optimizer, device=rank, status='eval')\n",
    "\n",
    "        data = data_t.mean(dim=0)\n",
    "        data_long = data_tl.mean(dim=0)\n",
    "\n",
    "        s = {}\n",
    "        s['test loss'] = data[-2]\n",
    "        # s['test acc.'] = data[-3]\n",
    "        s['norm weights squared'] = data[-1]\n",
    "\n",
    "        s['test loss long'] = data_long[-2]\n",
    "        # s['test acc. long'] = data_long[-3]\n",
    "        wandb.log(s,\n",
    "            step=epoch\n",
    "            )\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            outputFile = args.output_dir + '/model_n{!s}_s{!s}_w{!s}_epoch{!s}'.format(n_layers, split, weight_decay, epoch)\n",
    "\n",
    "            torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                    }, outputFile)\n",
    "\n",
    "    # cleanup()\n",
    "    wandb.finish()\n",
    "\n",
    "    outputFile = args.output_dir + '/model_n{!s}_s{!s}_w{!s}_{!s}_final'.format(n_layers, split, weight_decay, args.ID)\n",
    "\n",
    "    torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }, outputFile)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating data ---\n",
      "--- Finished generating data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8rmwi756) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e558aed9f5b48ceb5514f801a376384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training loss</td><td>1.97286</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-donkey-70</strong> at: <a href='https://wandb.ai/iasai/Generative%20Addition/runs/8rmwi756' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition/runs/8rmwi756</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231217_214748-8rmwi756/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8rmwi756). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kruthoff/addition_project/wandb/run-20231217_221833-nb4rciya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iasai/Generative%20Addition/runs/nb4rciya' target=\"_blank\">eager-dragon-71</a></strong> to <a href='https://wandb.ai/iasai/Generative%20Addition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iasai/Generative%20Addition' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iasai/Generative%20Addition/runs/nb4rciya' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition/runs/nb4rciya</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- 0 ---\n",
      "---train loss ---\n",
      "3.129304885864258\n",
      "---train loss ---\n",
      "2.087799549102783\n",
      "---train loss ---\n",
      "2.005053758621216\n",
      "---train loss ---\n",
      "1.8962360620498657\n",
      "---train loss ---\n",
      "2.0656561851501465\n",
      "---train loss ---\n",
      "1.9396512508392334\n",
      "---train loss ---\n",
      "2.0247485637664795\n",
      "---train loss ---\n",
      "1.8661630153656006\n",
      "---train loss ---\n",
      "1.8286575078964233\n",
      "---train loss ---\n",
      "1.9586646556854248\n",
      "---eval loss ---\n",
      "1.8151261806488037\n",
      "---eval loss ---\n",
      "4.8677144050598145\n",
      " --- 1 ---\n",
      "---train loss ---\n",
      "1.7702956199645996\n",
      "---train loss ---\n",
      "1.7478282451629639\n",
      "---train loss ---\n",
      "1.6920605897903442\n",
      "---train loss ---\n",
      "1.6502399444580078\n",
      "---train loss ---\n",
      "1.5660897493362427\n",
      "---train loss ---\n",
      "1.482843279838562\n",
      "---train loss ---\n",
      "1.6153030395507812\n",
      "---train loss ---\n",
      "1.553171992301941\n",
      "---train loss ---\n",
      "1.3898195028305054\n",
      "---train loss ---\n",
      "1.313326120376587\n",
      "---eval loss ---\n",
      "1.2403157949447632\n",
      "---eval loss ---\n",
      "3.1966092586517334\n",
      " --- 2 ---\n",
      "---train loss ---\n",
      "1.1400535106658936\n",
      "---train loss ---\n",
      "1.124019980430603\n",
      "---train loss ---\n",
      "0.9911548495292664\n",
      "---train loss ---\n",
      "0.7156718373298645\n",
      "---train loss ---\n",
      "0.6090861558914185\n",
      "---train loss ---\n",
      "0.5712595582008362\n",
      "---train loss ---\n",
      "0.7801601886749268\n",
      "---train loss ---\n",
      "0.4811856150627136\n",
      "---train loss ---\n",
      "0.5655961632728577\n",
      "---train loss ---\n",
      "0.35616493225097656\n",
      "---eval loss ---\n",
      "0.35592350363731384\n",
      "---eval loss ---\n",
      "4.372904300689697\n",
      " --- 3 ---\n",
      "---train loss ---\n",
      "0.3863728940486908\n",
      "---train loss ---\n",
      "0.26023370027542114\n",
      "---train loss ---\n",
      "0.29671788215637207\n",
      "---train loss ---\n",
      "0.1852477341890335\n",
      "---train loss ---\n",
      "0.21734803915023804\n",
      "---train loss ---\n",
      "0.43293190002441406\n",
      "---train loss ---\n",
      "0.3441694974899292\n",
      "---train loss ---\n",
      "0.22667576372623444\n",
      "---train loss ---\n",
      "0.16708926856517792\n",
      "---train loss ---\n",
      "0.14562039077281952\n",
      "---eval loss ---\n",
      "0.20424850285053253\n",
      "---eval loss ---\n",
      "5.033158302307129\n",
      " --- 4 ---\n",
      "---train loss ---\n",
      "0.24314619600772858\n",
      "---train loss ---\n",
      "0.16200460493564606\n",
      "---train loss ---\n",
      "0.1327233761548996\n",
      "---train loss ---\n",
      "0.22276298701763153\n",
      "---train loss ---\n",
      "0.1295703500509262\n",
      "---train loss ---\n",
      "0.1328750103712082\n",
      "---train loss ---\n",
      "0.25379395484924316\n",
      "---train loss ---\n",
      "0.08958639949560165\n",
      "---train loss ---\n",
      "0.22892409563064575\n",
      "---train loss ---\n",
      "0.15116222202777863\n",
      "---eval loss ---\n",
      "0.08522915095090866\n",
      "---eval loss ---\n",
      "5.9233012199401855\n",
      " --- 5 ---\n",
      "---train loss ---\n",
      "0.09712619334459305\n",
      "---train loss ---\n",
      "0.15438340604305267\n",
      "---train loss ---\n",
      "0.11253704875707626\n",
      "---train loss ---\n",
      "0.10975157469511032\n",
      "---train loss ---\n",
      "0.15523307025432587\n",
      "---train loss ---\n",
      "0.06987739354372025\n",
      "---train loss ---\n",
      "0.16625754535198212\n",
      "---train loss ---\n",
      "0.11615940183401108\n",
      "---train loss ---\n",
      "0.1313219964504242\n",
      "---train loss ---\n",
      "0.08294321596622467\n",
      "---eval loss ---\n",
      "0.09859448671340942\n",
      "---eval loss ---\n",
      "6.941245079040527\n",
      " --- 6 ---\n",
      "---train loss ---\n",
      "0.07810883969068527\n",
      "---train loss ---\n",
      "0.058545421808958054\n",
      "---train loss ---\n",
      "0.12435748428106308\n",
      "---train loss ---\n",
      "0.13748706877231598\n",
      "---train loss ---\n",
      "0.0980362668633461\n",
      "---train loss ---\n",
      "0.10357643663883209\n",
      "---train loss ---\n",
      "0.08146053552627563\n",
      "---train loss ---\n",
      "0.18539534509181976\n",
      "---train loss ---\n",
      "0.10229279845952988\n",
      "---train loss ---\n",
      "0.14450503885746002\n",
      "---eval loss ---\n",
      "0.09116678684949875\n",
      "---eval loss ---\n",
      "4.784368515014648\n",
      " --- 7 ---\n",
      "---train loss ---\n",
      "0.11603030562400818\n",
      "---train loss ---\n",
      "0.15953004360198975\n",
      "---train loss ---\n",
      "0.14645332098007202\n",
      "---train loss ---\n",
      "0.06395301967859268\n",
      "---train loss ---\n",
      "0.05116984620690346\n",
      "---train loss ---\n",
      "0.12210898846387863\n",
      "---train loss ---\n",
      "0.13294290006160736\n",
      "---train loss ---\n",
      "0.09650283306837082\n",
      "---train loss ---\n",
      "0.10545863956212997\n",
      "---train loss ---\n",
      "0.0895286574959755\n",
      "---eval loss ---\n",
      "0.06786949932575226\n",
      "---eval loss ---\n",
      "5.225967884063721\n",
      " --- 8 ---\n",
      "---train loss ---\n",
      "0.037820953875780106\n",
      "---train loss ---\n",
      "0.0817549005150795\n",
      "---train loss ---\n",
      "0.08839321881532669\n",
      "---train loss ---\n",
      "0.061856430023908615\n",
      "---train loss ---\n",
      "0.0636056438088417\n",
      "---train loss ---\n",
      "0.16568152606487274\n",
      "---train loss ---\n",
      "0.15486179292201996\n",
      "---train loss ---\n",
      "0.06648583710193634\n",
      "---train loss ---\n",
      "0.09048233926296234\n",
      "---train loss ---\n",
      "0.10864204168319702\n",
      "---eval loss ---\n",
      "0.06809970736503601\n",
      "---eval loss ---\n",
      "4.962797164916992\n",
      " --- 9 ---\n",
      "---train loss ---\n",
      "0.06043870747089386\n",
      "---train loss ---\n",
      "0.06992235779762268\n",
      "---train loss ---\n",
      "0.059461627155542374\n",
      "---train loss ---\n",
      "0.07427863776683807\n",
      "---train loss ---\n",
      "0.1488562822341919\n",
      "---train loss ---\n",
      "0.1416868418455124\n",
      "---train loss ---\n",
      "0.09444069862365723\n",
      "---train loss ---\n",
      "0.052495889365673065\n",
      "---train loss ---\n",
      "0.09615731984376907\n",
      "---train loss ---\n",
      "0.06885488331317902\n",
      "---eval loss ---\n",
      "0.06615997105836868\n",
      "---eval loss ---\n",
      "4.118517875671387\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1806ee832f41f9bded3f66a70112b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.284355…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>norm weights squared</td><td>▄█▅▄▄▄▃▂▂▁</td></tr><tr><td>test loss</td><td>█▆▂▂▁▁▁▁▁▁</td></tr><tr><td>test loss long</td><td>▄▁▃▄▆█▄▅▄▃</td></tr><tr><td>training loss</td><td>█▆▃▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>norm weights squared</td><td>17812.95117</td></tr><tr><td>test loss</td><td>0.06109</td></tr><tr><td>test loss long</td><td>4.11234</td></tr><tr><td>training loss</td><td>0.13046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-dragon-71</strong> at: <a href='https://wandb.ai/iasai/Generative%20Addition/runs/nb4rciya' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition/runs/nb4rciya</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231217_221833-nb4rciya/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--split\", default=0.3, type=float)\n",
    "parser.add_argument(\"--weight_decay\", default=0.3, type=float)\n",
    "parser.add_argument(\"--num_layers\", default=6, type=int)\n",
    "parser.add_argument(\"--d_model\", default=512, type=int)\n",
    "parser.add_argument(\"--d_ff\", default=2048, type=int)\n",
    "parser.add_argument(\"--heads\", default=8, type=int)\n",
    "parser.add_argument(\"--epochs\", default=10, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "parser.add_argument(\"--batch_size_eval\", default=1024, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float)\n",
    "parser.add_argument(\"--ndigits\", default=5, type=int)\n",
    "parser.add_argument(\"--nextra\", default=15, type=int)\n",
    "parser.add_argument(\"--output_dir\", default=\"generative/\", type=str)\n",
    "parser.add_argument(\"--ID\", default=16, type=int)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data, target, data_f, target_f = GenerateDataset_pp(5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'generative/model_n6_s0.3_w0.3_15_final'\n",
    "n_layer = 6 # number of layers\n",
    "d_model = 512 # model dimension, residual stream\n",
    "d_ff = 2048 # dim intermediate feed-forward layer\n",
    "h_a = 8 # number of heads in attention (doesnt impact # of params)\n",
    "\n",
    "model = make_model(vocab=13, N = n_layer, d_model = d_model, d_ff = d_ff, h = h_a)\n",
    "\n",
    "model.load_state_dict(torch.load(output_file)['model'])\n",
    "\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  1,  3,  4,  3, 11], device='cuda:0') tensor([ 4,  1,  3,  4,  3, 11], device='cuda:0')\n",
      "tensor([ 1,  7,  9,  4,  0, 11], device='cuda:0') tensor([ 1,  7,  9,  4,  0, 11], device='cuda:0')\n",
      "tensor([ 8,  3,  5, 11], device='cuda:0') tensor([ 8,  3,  5, 11], device='cuda:0')\n",
      "tensor([ 3,  7,  4,  8, 11], device='cuda:0') tensor([ 3,  7,  4,  8, 11], device='cuda:0')\n",
      "tensor([ 4,  2,  5,  0,  2, 11], device='cuda:0') tensor([ 4,  2,  5,  0,  2, 11], device='cuda:0')\n",
      "tensor([ 9,  9,  4, 11], device='cuda:0') tensor([ 9,  9,  4, 11], device='cuda:0')\n",
      "tensor([ 1,  3, 11], device='cuda:0') tensor([ 1,  3, 11], device='cuda:0')\n",
      "tensor([ 3,  7,  5,  4, 11], device='cuda:0') tensor([ 3,  7,  5,  4, 11], device='cuda:0')\n",
      "tensor([ 2,  1,  3,  1, 11], device='cuda:0') tensor([ 2,  1,  3,  1, 11], device='cuda:0')\n",
      "tensor([ 1,  5,  8,  0, 11], device='cuda:0') tensor([ 1,  5,  8,  0, 11], device='cuda:0')\n",
      "tensor([ 3,  8,  6,  3, 11], device='cuda:0') tensor([ 3,  8,  6,  3, 11], device='cuda:0')\n",
      "tensor([ 3,  7,  7,  1, 11], device='cuda:0') tensor([ 3,  7,  7,  1, 11], device='cuda:0')\n",
      "tensor([ 4,  1,  8,  2,  7, 11], device='cuda:0') tensor([ 4,  1,  8,  2,  7, 11], device='cuda:0')\n",
      "tensor([ 6,  1,  4,  4,  2, 11], device='cuda:0') tensor([ 6,  1,  4,  4,  2, 11], device='cuda:0')\n",
      "tensor([ 1,  0,  0, 11], device='cuda:0') tensor([ 5,  0, 11], device='cuda:0')\n",
      "tensor([ 1,  2,  9,  2,  6, 11], device='cuda:0') tensor([ 1,  1,  9,  2,  6, 11], device='cuda:0')\n",
      "tensor([ 1,  2,  7,  7,  3, 11], device='cuda:0') tensor([ 1,  2,  7,  7,  3, 11], device='cuda:0')\n",
      "tensor([ 3,  9,  6,  0,  8, 11], device='cuda:0') tensor([ 3,  9,  7,  0,  8, 11], device='cuda:0')\n",
      "tensor([ 4,  2,  2,  2, 11], device='cuda:0') tensor([ 4,  2,  2,  2, 11], device='cuda:0')\n",
      "tensor([ 5,  9,  8,  3, 11], device='cuda:0') tensor([ 5,  9,  8,  3, 11], device='cuda:0')\n",
      "tensor([ 2,  1, 11], device='cuda:0') tensor([ 2,  1, 11], device='cuda:0')\n",
      "tensor([ 3,  3, 11], device='cuda:0') tensor([ 3,  3, 11], device='cuda:0')\n",
      "tensor([ 7,  5,  2, 11], device='cuda:0') tensor([ 7,  5,  2, 11], device='cuda:0')\n",
      "tensor([ 9,  3,  6,  6,  4, 11], device='cuda:0') tensor([ 9,  3,  6,  6,  4, 11], device='cuda:0')\n",
      "tensor([ 6,  0,  6,  3, 11], device='cuda:0') tensor([ 6,  0,  6,  3, 11], device='cuda:0')\n",
      "tensor([ 1,  4,  3,  0,  4,  6, 11], device='cuda:0') tensor([ 1,  0,  4,  0,  4,  6, 11], device='cuda:0')\n",
      "tensor([ 4,  0, 11], device='cuda:0') tensor([ 4,  0, 11], device='cuda:0')\n",
      "tensor([ 6,  1,  8,  1,  3, 11], device='cuda:0') tensor([ 6,  1,  8,  1,  3, 11], device='cuda:0')\n",
      "tensor([ 7,  0, 11], device='cuda:0') tensor([ 7,  0, 11], device='cuda:0')\n",
      "tensor([ 7,  4,  3,  0,  5, 11], device='cuda:0') tensor([ 7,  4,  3,  0,  5, 11], device='cuda:0')\n",
      "tensor([ 1,  8,  2,  9,  7, 11], device='cuda:0') tensor([ 1,  8,  2,  9,  7, 11], device='cuda:0')\n",
      "tensor([ 1,  1,  2,  3,  0, 11], device='cuda:0') tensor([ 1,  1,  2,  3,  0, 11], device='cuda:0')\n",
      "tensor([ 8,  8,  3, 11], device='cuda:0') tensor([ 8,  8,  3, 11], device='cuda:0')\n",
      "tensor([ 5, 11], device='cuda:0') tensor([ 5, 11], device='cuda:0')\n",
      "tensor([ 5,  0,  7, 11], device='cuda:0') tensor([ 5,  0,  7, 11], device='cuda:0')\n",
      "tensor([ 1,  1,  0, 11], device='cuda:0') tensor([ 1,  1,  0, 11], device='cuda:0')\n",
      "tensor([ 2,  7,  4,  0, 11], device='cuda:0') tensor([ 1,  2,  7,  4,  0, 11], device='cuda:0')\n",
      "tensor([ 9, 11], device='cuda:0') tensor([ 9, 11], device='cuda:0')\n",
      "tensor([ 9,  0,  8,  3,  7, 11], device='cuda:0') tensor([ 9,  0,  8,  3,  7, 11], device='cuda:0')\n",
      "tensor([ 7,  9,  9, 11], device='cuda:0') tensor([ 7,  9,  9, 11], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ntot = 20\n",
    "\n",
    "acc = []\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    num_ex = 20\n",
    "    data_ff = data[j*num_ex:(j+1)*num_ex, :2*ntot + 1].to('cuda:0')\n",
    "\n",
    "    u = 0\n",
    "    inputs = data_ff\n",
    "    target_ff = target[j*num_ex:(j+1)*num_ex].to('cuda:0')\n",
    "    max_gen = 0\n",
    "\n",
    "    for i in range(inputs.shape[0]):\n",
    "        done = False\n",
    "        \n",
    "        src = inputs[i].unsqueeze(0)\n",
    "        tgt = target_ff[i]\n",
    "\n",
    "        while not done:\n",
    "            seq_len = src.shape[-1]\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len)).to('cuda:0')\n",
    "\n",
    "            out = model(src, mask)[:, -1]\n",
    "            next_tok = torch.argmax(out, -1)\n",
    "\n",
    "            max_gen += 1\n",
    "            src = torch.cat((src, next_tok.unsqueeze(1)), 1)\n",
    "\n",
    "            if (next_tok == 11) or (max_gen == ntot + 1):\n",
    "                done = True\n",
    "\n",
    "            if done:\n",
    "                src = src.squeeze()\n",
    "                in_z = src[-(ntot + 1):][src[-(ntot + 1):] != 12]\n",
    "                target_z = tgt[-(ntot + 1):][tgt[-(ntot + 1):] != 12]\n",
    "                print(in_z, target_z)\n",
    "                try:\n",
    "                    acc_z = (in_z == target_z).float().min().item()\n",
    "\n",
    "                    acc.append(acc_z)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "acc = torch.tensor(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9211)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb = model.embed.Emb.weight.data.detach().cpu()\n",
    "\n",
    "svdEMB = torch.svd(emb)\n",
    "\n",
    "a, b = 0, 1\n",
    "\n",
    "x = svdEMB[0][:, a] * svdEMB[1][a]\n",
    "y = svdEMB[0][:, b] * svdEMB[1][b]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "i = 0\n",
    "for x, y in zip(x, y):\n",
    "    plt.annotate(f'{i}', (x, y))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f67ba0d0610>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "att = model.decoder.layers[5].attn.attn[0, 1].detach().cpu()\n",
    "\n",
    "plt.imshow(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
