{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "from requests import get\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDataset_p(ndig, nextra):\n",
    "\n",
    "    stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11}\n",
    "\n",
    "    data = []\n",
    "    target = []\n",
    "    data_f = []\n",
    "    target_f = []\n",
    "\n",
    "    k = 0\n",
    "    while k < 200000:\n",
    "        i = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "        j = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        if len(li) < ndig+nextra:\n",
    "            li = ['0'] * (ndig+nextra - len(li)) + li\n",
    "        if len(lj) < ndig+nextra:\n",
    "            lj = ['0'] * (ndig+nextra - len(lj)) + lj\n",
    "        if len(lij) < ndig+nextra:\n",
    "            lij = ['0'] * (ndig+nextra - len(lij)) + lij\n",
    "\n",
    "        lsum = li + ['+'] + lj + lij\n",
    "        lt = lsum[1:] + ['=']\n",
    "        data.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "\n",
    "        include = False\n",
    "        while not include:\n",
    "            i = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            j = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            include = (i + j < 10**(ndig+nextra))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        if len(li) < ndig+nextra:\n",
    "            li = ['0'] * (ndig+nextra - len(li)) + li\n",
    "        if len(lj) < ndig+nextra:\n",
    "            lj = ['0'] * (ndig+nextra - len(lj)) + lj\n",
    "        if len(lij) < ndig+nextra:\n",
    "            lij = ['0'] * (ndig+nextra - len(lij)) + lij\n",
    "\n",
    "        lsum = li + ['+'] + lj + lij\n",
    "        lt = lsum[1:] + ['=']\n",
    "        data_f.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target_f.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "        k += 1\n",
    "\n",
    "    data_f = torch.LongTensor(data_f)\n",
    "    target_f = torch.LongTensor(target_f)\n",
    "    data = torch.LongTensor(data)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    vocab = len(stoi)\n",
    "    \n",
    "    return vocab, data, target, data_f, target_f\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, target):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src = self.inputs[index]\n",
    "        tgt = self.target[index]\n",
    "\n",
    "        return src, tgt\n",
    "    \n",
    "def prepare(rank, world_size, data, target, batch_size, pin_memory=True, num_workers=0):\n",
    "\n",
    "    dataset = Dataset(data, target)\n",
    "    # sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def run_epoch(data, loader, model, optimizer, device, status='train'):\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        src, tgt = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        seq_len = src.shape[-1]\n",
    "        num_digits = (seq_len - 1) // 3\n",
    "\n",
    "        logits = model.forward(src, num_last_tokens=num_digits+1).logits\n",
    "        \n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        tgt = tgt[:, -(num_digits + 1):]\n",
    "\n",
    "        kl_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = kl_loss(logits.transpose(-1, -2), tgt) # We want inputs to be (bs, vocab_size, seq len), so needed a transpose. Targets are (bs, seq len) with values in [0, vocab_size]\n",
    "\n",
    "        a = (torch.argmax(logits.detach(), dim=-1) == tgt).float()\n",
    "\n",
    "        acc_p = sum((torch.argmax(logits.detach(), dim=-1)[i] == tgt[i]).float().min() for i in range(len(tgt))) / len(tgt)\n",
    "\n",
    "        a = a.mean(dim=0).tolist()\n",
    "        a.append(acc_p)\n",
    "        a.append(loss.detach().item())\n",
    "\n",
    "        if status == 'train':\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if status == 'eval':\n",
    "            w2 = sum((p.data**2).sum() for p in model.parameters()).clone().detach().to('cpu')\n",
    "            a.append(w2.item())\n",
    "        \n",
    "        pre_data = torch.tensor(a) # accuracy per token, correctness, loss\n",
    "        data = torch.cat((data, pre_data.unsqueeze(0)), 0)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'---{status} loss ---')\n",
    "            print(loss.detach().item())\n",
    "            print(acc_p.detach().item())\n",
    "\n",
    "        del loss, tgt, src, logits\n",
    "        gc.collect\n",
    "\n",
    "    return data\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    ### Dataset\n",
    "\n",
    "    print('--- Generating data ---')\n",
    "    vocab, data, target, data_f, target_f = GenerateDataset_p(ndig=args.ndigits, nextra = args.nextra)\n",
    "    print('--- Finished generating data ---')\n",
    "    # Three way split (training, test)\n",
    "    random.seed()\n",
    "    z = list(zip(data.tolist(), target.tolist()))\n",
    "    random.shuffle(z)\n",
    "\n",
    "    z1, z2 = zip(*z)\n",
    "    src_array_sh, tgt_array_sh = torch.LongTensor(list(z1)), torch.LongTensor(list(z2))\n",
    "\n",
    "    rank = torch.device('cuda:0')\n",
    "    world_size = 0\n",
    "\n",
    "    # Dataset parameters\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    batch_size_eval = args.batch_size_eval\n",
    "\n",
    "    split = args.split\n",
    "\n",
    "    n1 = int(split*len(src_array_sh))\n",
    "    n2 = 2*n1\n",
    "    # n1 = 0\n",
    "    # n0 = 100\n",
    "    priming_examples = 0\n",
    "\n",
    "    src_train, src_test = src_array_sh[:n1], src_array_sh[n1:n2]\n",
    "    tgt_train, tgt_test = tgt_array_sh[:n1], tgt_array_sh[n1:n2]\n",
    "    src_long, tgt_long = data_f[:priming_examples], target_f[:priming_examples]\n",
    "    src_test_long, tgt_test_long = data_f[priming_examples:], target_f[priming_examples:]\n",
    "\n",
    "    # src_train = torch.cat((src_train, src_long), 0)\n",
    "    # tgt_train = torch.cat((tgt_train, tgt_long), 0)\n",
    "    # src_train = src_long\n",
    "    # tgt_train = tgt_long\n",
    "\n",
    "    random.seed()\n",
    "    z = list(zip(src_train.tolist(), tgt_train.tolist()))\n",
    "    random.shuffle(z)\n",
    "\n",
    "    z1, z2 = zip(*z)\n",
    "    src_train, tgt_train = torch.LongTensor(list(z1)), torch.LongTensor(list(z2))\n",
    "\n",
    "    dataloader_train = prepare(rank, world_size, src_train, tgt_train, batch_size)\n",
    "\n",
    "    dataloader_test = prepare(rank, world_size, src_test, tgt_test, batch_size_eval)\n",
    "\n",
    "    dataloader_test_long = prepare(rank, world_size, src_test_long, tgt_test_long, batch_size_eval)\n",
    "\n",
    "    ### Model\n",
    "\n",
    "    d_model = args.d_model\n",
    "    n_layer = args.num_layers\n",
    "\n",
    "    model = MambaLMHeadModel(d_model=d_model, n_layer=n_layer, vocab_size=vocab, device='cuda:0')  \n",
    "\n",
    "    model = model.to(rank)\n",
    "\n",
    "    ### Training parameters and optimizer\n",
    "\n",
    "    lr = args.learning_rate\n",
    "    weight_decay = args.weight_decay    \n",
    "    num_epochs = args.epochs\n",
    "\n",
    "    # warmup = 100\n",
    "    # lr_max = 1e-3\n",
    "    # lr_min = 3e-4\n",
    "    # lr_decay_steps = 10000\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr = lr,\n",
    "                                  betas = (0.9, 0.95),\n",
    "                                  eps=1e-8,\n",
    "                                  weight_decay=weight_decay)\n",
    "    \n",
    "    # lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda = lambda step: 1+0*(1/lr_max)*rate_cos(step, \n",
    "    #                                                                            warmup=warmup, \n",
    "    #                                                                            lr_max=lr_max, \n",
    "    #                                                                            lr_min=lr_min, \n",
    "    #                                                                            lr_decay_steps=lr_decay_steps) )\n",
    "\n",
    "    ### Tracking\n",
    "\n",
    "    wandb.init(project=\"Mamba arithmetic generative\", \n",
    "                     config={\"lr\": lr, \n",
    "                             \"split\":split, \n",
    "                             \"layers\": n_layer, \n",
    "                             \"weight decay\": weight_decay, \n",
    "                             \"d_model\": d_model,\n",
    "                             \"Priming examples\": priming_examples, \n",
    "                             \"Batch size train\": batch_size, \n",
    "                             \"Batch size eval\": batch_size_eval, \n",
    "                             'Task': f'{args.ndigits + args.nextra} digit addition sums', \n",
    "                             'ID': args.ID, \n",
    "                             \"num digits train\": args.ndigits})\n",
    "\n",
    "\n",
    "    ### Training    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f' --- {epoch} ---')\n",
    "        data = torch.tensor([])\n",
    "        data_t = torch.tensor([])\n",
    "        data_tl = torch.tensor([])\n",
    "\n",
    "        model.train()\n",
    "        data = run_epoch(data, loader=dataloader_train, model=model, optimizer=optimizer, device=rank, status='train')\n",
    "        \n",
    "        data = data.mean(dim=0)\n",
    "        s = {}\n",
    "        s['training loss'] = data[-1]\n",
    "        z = [f'training acc. pos {i}' for i in range(args.ndigits + args.nextra + 1)]\n",
    "        for i in range(len(z)):\n",
    "            s[z[i]] = data[i]\n",
    "        s['training acc.'] = data[-2].mean()\n",
    "        wandb.log(s,\n",
    "            step=epoch\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_t = run_epoch(data_t, loader=dataloader_test, model=model, optimizer=optimizer, device=rank, status='eval')\n",
    "\n",
    "            data_tl = run_epoch(data_tl, loader=dataloader_test_long, model=model, optimizer=optimizer, device=rank, status='eval')\n",
    "\n",
    "        data = data_t.mean(dim=0)\n",
    "        data_long = data_tl.mean(dim=0)\n",
    "\n",
    "        s = {}\n",
    "        s['test loss'] = data[-2]\n",
    "        z = [f'test acc. pos {i}' for i in range(args.ndigits + args.nextra + 1)]\n",
    "        for i in range(len(z)):\n",
    "            s[z[i]] = data[i]\n",
    "        s['test acc.'] = data[-3]\n",
    "        s['norm weights squared'] = data[-1]\n",
    "\n",
    "        s['test loss long'] = data_long[-2]\n",
    "        z = [f'test acc. pos {i} long' for i in range(args.ndigits + args.nextra + 1)]\n",
    "        for i in range(len(z)):\n",
    "            s[z[i]] = data_long[i]\n",
    "        s['test acc. long'] = data_long[-3]\n",
    "        wandb.log(s,\n",
    "            step=epoch\n",
    "            )\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            outputFile = args.output_dir + '/model_n{!s}_s{!s}_w{!s}_epoch{!s}'.format(n_layer, split, weight_decay, epoch)\n",
    "\n",
    "            torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                    }, outputFile)\n",
    "\n",
    "    # cleanup()\n",
    "    wandb.finish()\n",
    "\n",
    "    outputFile = args.output_dir + '/model_n{!s}_s{!s}_w{!s}_{!s}_final'.format(n_layer, split, weight_decay, args.ID)\n",
    "\n",
    "    torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }, outputFile)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating data ---\n",
      "--- Finished generating data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qtjni3y2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>norm weights squared</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test acc.</td><td>▁▄▄▇▇▇▇▇▇██▇█▇</td></tr><tr><td>test acc. long</td><td>▁▁▃█▃▃▄▂▂▄▂▁▅▂</td></tr><tr><td>test acc. pos 0</td><td>▁██▃▃██▃▃█████</td></tr><tr><td>test acc. pos 0 long</td><td>██▇▇▄▃▅▁▁▂▃▅▅▁</td></tr><tr><td>test acc. pos 1</td><td>████████▁█████</td></tr><tr><td>test acc. pos 1 long</td><td>▇▇▇█▂▃▅▂▃▂▂▆▇▁</td></tr><tr><td>test acc. pos 10</td><td>█▇█▂▇███▁█████</td></tr><tr><td>test acc. pos 10 long</td><td>▁▃▂▁▄▃▂▄▆▅▇▆▇█</td></tr><tr><td>test acc. pos 2</td><td>▇████▇▁███████</td></tr><tr><td>test acc. pos 2 long</td><td>█▅▆▇▅▅▆▄▃▃▄▄▄▁</td></tr><tr><td>test acc. pos 3</td><td>▇█▇██▇██▅██▇▁▃</td></tr><tr><td>test acc. pos 3 long</td><td>█▆▇▇▆▆▇▆▄▅▄▃▃▁</td></tr><tr><td>test acc. pos 4</td><td>▂▄▁█▄▇▄▇▆██▇▇▆</td></tr><tr><td>test acc. pos 4 long</td><td>█▆▇█▆▇▇▇▄▅▄▄▄▁</td></tr><tr><td>test acc. pos 5</td><td>▁▁▃▇▆▇▆█▇█▇▇▇▇</td></tr><tr><td>test acc. pos 5 long</td><td>▁▃▅█▅▇▅▇▆▇▆▄▆▅</td></tr><tr><td>test acc. pos 6</td><td>▁▄▃▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>test acc. pos 6 long</td><td>▃▄▆█▅▅▅▄▄▄▁▁▂▁</td></tr><tr><td>test acc. pos 7</td><td>▁▄▅▆▇▆▇▇▇▇█▇█▇</td></tr><tr><td>test acc. pos 7 long</td><td>▄▅█████▅▄▂▁▅▅▄</td></tr><tr><td>test acc. pos 8</td><td>▁▅▄▅▅▆▆▅▇▇███▆</td></tr><tr><td>test acc. pos 8 long</td><td>▄▆██▇▇█▆▄▅▁▆▅▄</td></tr><tr><td>test acc. pos 9</td><td>▁▅▆▆▆▇▇█▇▆▇▇█▅</td></tr><tr><td>test acc. pos 9 long</td><td>▄▆▇█▇▇▇▆▄▅▁▅▅▄</td></tr><tr><td>test loss</td><td>█▅▅▃▃▂▂▂▂▁▁▁▁▂</td></tr><tr><td>test loss long</td><td>▄▄▂▁▂▃▁▂▄▄▆▄▅█</td></tr><tr><td>training acc.</td><td>▁▅▇▇▇█████████</td></tr><tr><td>training acc. pos 0</td><td>▁█████████████</td></tr><tr><td>training acc. pos 1</td><td>▃▄▃▁▆██████▆█▄</td></tr><tr><td>training acc. pos 10</td><td>▁▇▇▇██████████</td></tr><tr><td>training acc. pos 2</td><td>▁█▇██▆▇▇▇█▇▆▇█</td></tr><tr><td>training acc. pos 3</td><td>▁▇▇▅▃▆▆▇▇▆█▅▆▆</td></tr><tr><td>training acc. pos 4</td><td>▁▄▅▆▇▇▇▇▇▇████</td></tr><tr><td>training acc. pos 5</td><td>▁▆▇▇██████████</td></tr><tr><td>training acc. pos 6</td><td>▁▆▇▇██████████</td></tr><tr><td>training acc. pos 7</td><td>▁▆▇▇██████████</td></tr><tr><td>training acc. pos 8</td><td>▁▆▇▇▇▇▇███████</td></tr><tr><td>training acc. pos 9</td><td>▁▇████████████</td></tr><tr><td>training loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>norm weights squared</td><td>2997.87671</td></tr><tr><td>test acc.</td><td>0.87878</td></tr><tr><td>test acc. long</td><td>0.00021</td></tr><tr><td>test acc. pos 0</td><td>1.0</td></tr><tr><td>test acc. pos 0 long</td><td>0.19751</td></tr><tr><td>test acc. pos 1</td><td>1.0</td></tr><tr><td>test acc. pos 1 long</td><td>0.1878</td></tr><tr><td>test acc. pos 10</td><td>0.99998</td></tr><tr><td>test acc. pos 10 long</td><td>0.9599</td></tr><tr><td>test acc. pos 2</td><td>0.99999</td></tr><tr><td>test acc. pos 2 long</td><td>0.15115</td></tr><tr><td>test acc. pos 3</td><td>0.9999</td></tr><tr><td>test acc. pos 3 long</td><td>0.06225</td></tr><tr><td>test acc. pos 4</td><td>0.99672</td></tr><tr><td>test acc. pos 4 long</td><td>0.04306</td></tr><tr><td>test acc. pos 5</td><td>0.98206</td></tr><tr><td>test acc. pos 5 long</td><td>0.2555</td></tr><tr><td>test acc. pos 6</td><td>0.96352</td></tr><tr><td>test acc. pos 6 long</td><td>0.15346</td></tr><tr><td>test acc. pos 7</td><td>0.96535</td></tr><tr><td>test acc. pos 7 long</td><td>0.30642</td></tr><tr><td>test acc. pos 8</td><td>0.95573</td></tr><tr><td>test acc. pos 8 long</td><td>0.19014</td></tr><tr><td>test acc. pos 9</td><td>0.97581</td></tr><tr><td>test acc. pos 9 long</td><td>0.14468</td></tr><tr><td>test loss</td><td>0.05017</td></tr><tr><td>test loss long</td><td>7.23988</td></tr><tr><td>training acc.</td><td>0.925</td></tr><tr><td>training acc. pos 0</td><td>0.99998</td></tr><tr><td>training acc. pos 1</td><td>0.99998</td></tr><tr><td>training acc. pos 10</td><td>0.9996</td></tr><tr><td>training acc. pos 2</td><td>0.99999</td></tr><tr><td>training acc. pos 3</td><td>0.99998</td></tr><tr><td>training acc. pos 4</td><td>0.9974</td></tr><tr><td>training acc. pos 5</td><td>0.98559</td></tr><tr><td>training acc. pos 6</td><td>0.97004</td></tr><tr><td>training acc. pos 7</td><td>0.97107</td></tr><tr><td>training acc. pos 8</td><td>0.98678</td></tr><tr><td>training acc. pos 9</td><td>0.98941</td></tr><tr><td>training loss</td><td>0.02898</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-snowball-11</strong> at: <a href='https://wandb.ai/iasai/Mamba%20arithmetic%20generative/runs/qtjni3y2' target=\"_blank\">https://wandb.ai/iasai/Mamba%20arithmetic%20generative/runs/qtjni3y2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231218_131541-qtjni3y2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qtjni3y2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e64fb81aa543fca0e1049c6d290d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668055466531464, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kruthoff/addition_project/wandb/run-20231218_133148-cl5n92j4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iasai/Mamba%20arithmetic%20generative/runs/cl5n92j4' target=\"_blank\">azure-thunder-12</a></strong> to <a href='https://wandb.ai/iasai/Mamba%20arithmetic%20generative' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iasai/Mamba%20arithmetic%20generative' target=\"_blank\">https://wandb.ai/iasai/Mamba%20arithmetic%20generative</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iasai/Mamba%20arithmetic%20generative/runs/cl5n92j4' target=\"_blank\">https://wandb.ai/iasai/Mamba%20arithmetic%20generative/runs/cl5n92j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- 0 ---\n",
      "---train loss ---\n",
      "2.1354007720947266\n",
      "0.0\n",
      "---train loss ---\n",
      "0.8087169528007507\n",
      "0.0\n",
      "---train loss ---\n",
      "0.8992887735366821\n",
      "0.03125\n",
      "---train loss ---\n",
      "0.7350298166275024\n",
      "0.0\n",
      "---train loss ---\n",
      "0.7109223008155823\n",
      "0.0\n",
      "---train loss ---\n",
      "0.6374480128288269\n",
      "0.03125\n",
      "---train loss ---\n",
      "0.6963624358177185\n",
      "0.0\n",
      "---train loss ---\n",
      "0.5609874725341797\n",
      "0.125\n",
      "---train loss ---\n",
      "0.5515245199203491\n",
      "0.03125\n",
      "---train loss ---\n",
      "0.5722057223320007\n",
      "0.03125\n",
      "---train loss ---\n",
      "0.48006173968315125\n",
      "0.15625\n",
      "---train loss ---\n",
      "0.47704583406448364\n",
      "0.125\n",
      "---train loss ---\n",
      "0.5175455212593079\n",
      "0.03125\n",
      "---train loss ---\n",
      "0.47143813967704773\n",
      "0.21875\n",
      "---train loss ---\n",
      "0.3276655972003937\n",
      "0.3125\n",
      "---train loss ---\n",
      "0.23708638548851013\n",
      "0.4375\n",
      "---train loss ---\n",
      "0.2848615050315857\n",
      "0.1875\n",
      "---train loss ---\n",
      "0.33853113651275635\n",
      "0.125\n",
      "---train loss ---\n",
      "0.21970447897911072\n",
      "0.40625\n",
      "---train loss ---\n",
      "0.18248699605464935\n",
      "0.5625\n",
      "---train loss ---\n",
      "0.2661783993244171\n",
      "0.34375\n",
      "---train loss ---\n",
      "0.217071995139122\n",
      "0.46875\n",
      "---train loss ---\n",
      "0.17270898818969727\n",
      "0.59375\n",
      "---train loss ---\n",
      "0.21274572610855103\n",
      "0.34375\n",
      "---train loss ---\n",
      "0.251557856798172\n",
      "0.1875\n",
      "---train loss ---\n",
      "0.21341095864772797\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.22403669357299805\n",
      "0.4375\n",
      "---train loss ---\n",
      "0.18917585909366608\n",
      "0.46875\n",
      "---train loss ---\n",
      "0.1967494636774063\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.19422346353530884\n",
      "0.28125\n",
      "---train loss ---\n",
      "0.23665548861026764\n",
      "0.5\n",
      "---train loss ---\n",
      "0.10517928749322891\n",
      "0.53125\n",
      "---eval loss ---\n",
      "0.14686262607574463\n",
      "0.5927734375\n",
      "---eval loss ---\n",
      "9.766266822814941\n",
      "0.0\n",
      "---eval loss ---\n",
      "9.439377784729004\n",
      "0.0\n",
      " --- 1 ---\n",
      "---train loss ---\n",
      "0.18744893372058868\n",
      "0.5\n",
      "---train loss ---\n",
      "0.18612712621688843\n",
      "0.59375\n",
      "---train loss ---\n",
      "0.18107901513576508\n",
      "0.46875\n",
      "---train loss ---\n",
      "0.1133318617939949\n",
      "0.46875\n",
      "---train loss ---\n",
      "0.11473946273326874\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.126963809132576\n",
      "0.59375\n",
      "---train loss ---\n",
      "0.14525823295116425\n",
      "0.6875\n",
      "---train loss ---\n",
      "0.16348083317279816\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.08222131431102753\n",
      "0.75\n",
      "---train loss ---\n",
      "0.1382782757282257\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.11751516163349152\n",
      "0.6875\n",
      "---train loss ---\n",
      "0.1474428027868271\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.12316884845495224\n",
      "0.625\n",
      "---train loss ---\n",
      "0.1825108528137207\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.13729912042617798\n",
      "0.59375\n",
      "---train loss ---\n",
      "0.08381355553865433\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.08419257402420044\n",
      "0.6875\n",
      "---train loss ---\n",
      "0.10736921429634094\n",
      "0.65625\n",
      "---train loss ---\n",
      "0.146097794175148\n",
      "0.59375\n",
      "---train loss ---\n",
      "0.1209237352013588\n",
      "0.75\n",
      "---train loss ---\n",
      "0.12072651088237762\n",
      "0.75\n",
      "---train loss ---\n",
      "0.12549863755702972\n",
      "0.71875\n",
      "---train loss ---\n",
      "0.06462372094392776\n",
      "0.71875\n",
      "---train loss ---\n",
      "0.07350822538137436\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.10948500782251358\n",
      "0.75\n",
      "---train loss ---\n",
      "0.09043523669242859\n",
      "0.71875\n",
      "---train loss ---\n",
      "0.13335908949375153\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.05267417058348656\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.08028215914964676\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.0822969451546669\n",
      "0.75\n",
      "---train loss ---\n",
      "0.07221081852912903\n",
      "0.71875\n",
      "---train loss ---\n",
      "0.05520406737923622\n",
      "0.78125\n",
      "---eval loss ---\n",
      "0.09851273894309998\n",
      "0.7158203125\n",
      "---eval loss ---\n",
      "11.93343734741211\n",
      "0.0\n",
      "---eval loss ---\n",
      "11.782419204711914\n",
      "0.0\n",
      " --- 2 ---\n",
      "---train loss ---\n",
      "0.15770787000656128\n",
      "0.65625\n",
      "---train loss ---\n",
      "0.06901898235082626\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.05955972522497177\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.03237362951040268\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.08483623713254929\n",
      "0.75\n",
      "---train loss ---\n",
      "0.06466850638389587\n",
      "0.6875\n",
      "---train loss ---\n",
      "0.056553877890110016\n",
      "0.875\n",
      "---train loss ---\n",
      "0.046560775488615036\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.04866500943899155\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.06690087169408798\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.053759995847940445\n",
      "0.875\n",
      "---train loss ---\n",
      "0.07498422265052795\n",
      "0.75\n",
      "---train loss ---\n",
      "0.04059070348739624\n",
      "0.875\n",
      "---train loss ---\n",
      "0.1339644193649292\n",
      "0.71875\n",
      "---train loss ---\n",
      "0.14124880731105804\n",
      "0.53125\n",
      "---train loss ---\n",
      "0.04279138892889023\n",
      "0.875\n",
      "---train loss ---\n",
      "0.06332302838563919\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.07967784255743027\n",
      "0.75\n",
      "---train loss ---\n",
      "0.05095313861966133\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.061734188348054886\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.07932988554239273\n",
      "0.6875\n",
      "---train loss ---\n",
      "0.06919380277395248\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.05597614124417305\n",
      "0.75\n",
      "---train loss ---\n",
      "0.057441726326942444\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.04433629661798477\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.05240282043814659\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.0912022516131401\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.03466349467635155\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.07866337150335312\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.06263139843940735\n",
      "0.875\n",
      "---train loss ---\n",
      "0.06393665820360184\n",
      "0.75\n",
      "---train loss ---\n",
      "0.07864426076412201\n",
      "0.75\n",
      "---eval loss ---\n",
      "0.04786744341254234\n",
      "0.8525390625\n",
      "---eval loss ---\n",
      "14.542403221130371\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "14.250746726989746\n",
      "0.0\n",
      " --- 3 ---\n",
      "---train loss ---\n",
      "0.06965040415525436\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.028479663655161858\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.09037020802497864\n",
      "0.65625\n",
      "---train loss ---\n",
      "0.018031025305390358\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.02394997514784336\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.018945354968309402\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.06812906265258789\n",
      "0.875\n",
      "---train loss ---\n",
      "0.015252945013344288\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.01882448047399521\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.06691008806228638\n",
      "0.75\n",
      "---train loss ---\n",
      "0.03388849273324013\n",
      "0.875\n",
      "---train loss ---\n",
      "0.05200387164950371\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.07889252156019211\n",
      "0.75\n",
      "---train loss ---\n",
      "0.07489442080259323\n",
      "0.6875\n",
      "---train loss ---\n",
      "0.033366743475198746\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03540734946727753\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.019259052351117134\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.024029122665524483\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03711455687880516\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.05201951041817665\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.02587144449353218\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0451151542365551\n",
      "0.875\n",
      "---train loss ---\n",
      "0.02460489422082901\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.022658150643110275\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.019637422636151314\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03057646006345749\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.06593528389930725\n",
      "0.875\n",
      "---train loss ---\n",
      "0.02252333052456379\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.029658524319529533\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03826875984668732\n",
      "0.875\n",
      "---train loss ---\n",
      "0.01482915598899126\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.015861082822084427\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.04087328910827637\n",
      "0.890625\n",
      "---eval loss ---\n",
      "15.43044376373291\n",
      "0.0\n",
      "---eval loss ---\n",
      "15.046469688415527\n",
      "0.0\n",
      " --- 4 ---\n",
      "---train loss ---\n",
      "0.09480686485767365\n",
      "0.78125\n",
      "---train loss ---\n",
      "0.03919301927089691\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.11514002829790115\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.032200466841459274\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.015630904585123062\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03082899935543537\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.010528980754315853\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.01911143586039543\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.017853885889053345\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.013722816482186317\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.018050050362944603\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.034725114703178406\n",
      "0.875\n",
      "---train loss ---\n",
      "0.028665900230407715\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.03161020949482918\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.041669175028800964\n",
      "0.875\n",
      "---train loss ---\n",
      "0.011366325430572033\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.01783263497054577\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.02285824902355671\n",
      "0.875\n",
      "---train loss ---\n",
      "0.0419035479426384\n",
      "0.875\n",
      "---train loss ---\n",
      "0.03190029039978981\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03226697817444801\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.011581536382436752\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.009442735463380814\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.027352241799235344\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.017661690711975098\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.010435703210532665\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.033813659101724625\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.020220817998051643\n",
      "0.875\n",
      "---train loss ---\n",
      "0.01758398860692978\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0297405906021595\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.010780069045722485\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.01046393159776926\n",
      "0.9375\n",
      "---eval loss ---\n",
      "0.017272355034947395\n",
      "0.943359375\n",
      "---eval loss ---\n",
      "16.589372634887695\n",
      "0.0\n",
      "---eval loss ---\n",
      "16.490922927856445\n",
      "0.0009765625\n",
      " --- 5 ---\n",
      "---train loss ---\n",
      "0.015880363062024117\n",
      "1.0\n",
      "---train loss ---\n",
      "0.017695050686597824\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.08187416940927505\n",
      "0.875\n",
      "---train loss ---\n",
      "0.004746806342154741\n",
      "1.0\n",
      "---train loss ---\n",
      "0.016688641160726547\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.004924701526761055\n",
      "1.0\n",
      "---train loss ---\n",
      "0.022421730682253838\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.026731491088867188\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00941270962357521\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.018947716802358627\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.031214850023388863\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.006089320871978998\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01473626121878624\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.026496142148971558\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.040076956152915955\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.016581272706389427\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.009940273128449917\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.013680650852620602\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.019611025229096413\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0066182273440063\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.03654715418815613\n",
      "0.8125\n",
      "---train loss ---\n",
      "0.015137487091124058\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.018063709139823914\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.009954781271517277\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.017637236043810844\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.016687598079442978\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.02203969657421112\n",
      "0.875\n",
      "---train loss ---\n",
      "0.0071755810640752316\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007809283211827278\n",
      "1.0\n",
      "---train loss ---\n",
      "0.029731232672929764\n",
      "0.875\n",
      "---train loss ---\n",
      "0.011594736017286777\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.003376629902049899\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.009350077249109745\n",
      "0.966796875\n",
      "---eval loss ---\n",
      "19.265296936035156\n",
      "0.0\n",
      "---eval loss ---\n",
      "18.754920959472656\n",
      "0.00390625\n",
      " --- 6 ---\n",
      "---train loss ---\n",
      "0.004876851569861174\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.016091644763946533\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.023227030411362648\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0061344425193965435\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.003495772136375308\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004853484220802784\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004201031755656004\n",
      "1.0\n",
      "---train loss ---\n",
      "0.05865699052810669\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.01110310759395361\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005283741280436516\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0034095330629497766\n",
      "1.0\n",
      "---train loss ---\n",
      "0.02890748344361782\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.009364468976855278\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.025085868313908577\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.005697896704077721\n",
      "1.0\n",
      "---train loss ---\n",
      "0.02438041754066944\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.006618065759539604\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011547088623046875\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03765399754047394\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0119733065366745\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.022356752306222916\n",
      "0.875\n",
      "---train loss ---\n",
      "0.029930293560028076\n",
      "0.875\n",
      "---train loss ---\n",
      "0.004348837770521641\n",
      "1.0\n",
      "---train loss ---\n",
      "0.025773631408810616\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.006986056454479694\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009387084282934666\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005159303080290556\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0029157616663724184\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0068469881080091\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.017082275822758675\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.03649618849158287\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0025449967943131924\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.011630029417574406\n",
      "0.9638671875\n",
      "---eval loss ---\n",
      "15.213113784790039\n",
      "0.0029296875\n",
      "---eval loss ---\n",
      "14.836526870727539\n",
      "0.0009765625\n",
      " --- 7 ---\n",
      "---train loss ---\n",
      "0.028973938897252083\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.004229407291859388\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009837443009018898\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0036733984015882015\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002715078881010413\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011768230237066746\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00783231295645237\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.03573045879602432\n",
      "0.84375\n",
      "---train loss ---\n",
      "0.0016659797402098775\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004287282004952431\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0017060681711882353\n",
      "1.0\n",
      "---train loss ---\n",
      "0.013786882162094116\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.010234654881060123\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.011728399433195591\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.015836182981729507\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.014504627324640751\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.01640694960951805\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.007254607975482941\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.010555990040302277\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.009443862363696098\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.005797077436000109\n",
      "1.0\n",
      "---train loss ---\n",
      "0.008104435168206692\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0029300854075700045\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006506652105599642\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.031714022159576416\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.013061653822660446\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0133000947535038\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0018119268352165818\n",
      "1.0\n",
      "---train loss ---\n",
      "0.010492383502423763\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.023356882855296135\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0012973708799108863\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0056038107722997665\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.0158186424523592\n",
      "0.958984375\n",
      "---eval loss ---\n",
      "13.29836654663086\n",
      "0.001953125\n",
      "---eval loss ---\n",
      "12.910680770874023\n",
      "0.0029296875\n",
      " --- 8 ---\n",
      "---train loss ---\n",
      "0.046180903911590576\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.007452835328876972\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.004236645996570587\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000884634384419769\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004858171567320824\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00671251118183136\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.028924591839313507\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.024855906143784523\n",
      "0.875\n",
      "---train loss ---\n",
      "0.0030978566501289606\n",
      "1.0\n",
      "---train loss ---\n",
      "0.015804920345544815\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0012948049698024988\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007310894317924976\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.013060019351541996\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.014820280484855175\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.013813767582178116\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.008803270757198334\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007572807371616364\n",
      "1.0\n",
      "---train loss ---\n",
      "0.026081234216690063\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.03166699782013893\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0008472339832223952\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009282915852963924\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.022643812000751495\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.004603581968694925\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.014299299567937851\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.008515835739672184\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0019605434499680996\n",
      "1.0\n",
      "---train loss ---\n",
      "0.014863141812384129\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.001796272466890514\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01672421209514141\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.029893629252910614\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.006871487013995647\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007459468906745315\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.008578919805586338\n",
      "0.974609375\n",
      "---eval loss ---\n",
      "16.38170623779297\n",
      "0.0029296875\n",
      "---eval loss ---\n",
      "15.815146446228027\n",
      "0.001953125\n",
      " --- 9 ---\n",
      "---train loss ---\n",
      "0.019257722422480583\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.002809157595038414\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0032545537687838078\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010571791790425777\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0062050954438745975\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.010492687113583088\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.008897718973457813\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0059730289503932\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0025223661214113235\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006232117302715778\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002088325098156929\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002049061469733715\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0030953253153711557\n",
      "1.0\n",
      "---train loss ---\n",
      "0.043275460600852966\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.013534296303987503\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0015671744477003813\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003962524235248566\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005513402633368969\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.006475046277046204\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0032043708488345146\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.003502853913232684\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011199722066521645\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0012835251400247216\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0033812960609793663\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005297399591654539\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.004057692363858223\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0182475745677948\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0004871509736403823\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0022592844907194376\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0037049187812954187\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0019487396348267794\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005819904617965221\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.0077261850237846375\n",
      "0.9736328125\n",
      "---eval loss ---\n",
      "14.496565818786621\n",
      "0.0048828125\n",
      "---eval loss ---\n",
      "14.279926300048828\n",
      "0.005859375\n",
      " --- 10 ---\n",
      "---train loss ---\n",
      "0.006865808740258217\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0012001964496448636\n",
      "1.0\n",
      "---train loss ---\n",
      "0.11114251613616943\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.008979198522865772\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0010660127736628056\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009243473177775741\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001380244386382401\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003737518796697259\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0011842052917927504\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0063391816802322865\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.011153140105307102\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0026093660853803158\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003088901052251458\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0044136312790215015\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006245210766792297\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.010998734273016453\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0016283891163766384\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001928585348650813\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011850445531308651\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00042840559035539627\n",
      "1.0\n",
      "---train loss ---\n",
      "0.021194104105234146\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0023334629368036985\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010742879239842296\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002024206332862377\n",
      "1.0\n",
      "---train loss ---\n",
      "0.010768412612378597\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0006712673348374665\n",
      "1.0\n",
      "---train loss ---\n",
      "0.012543749064207077\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.001096158055588603\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0011411031009629369\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006510622799396515\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00042564672185108066\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007293872768059373\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.00406953739002347\n",
      "0.9833984375\n",
      "---eval loss ---\n",
      "18.055814743041992\n",
      "0.001953125\n",
      "---eval loss ---\n",
      "17.699201583862305\n",
      "0.0029296875\n",
      " --- 11 ---\n",
      "---train loss ---\n",
      "0.023152995854616165\n",
      "0.875\n",
      "---train loss ---\n",
      "0.001762042986229062\n",
      "1.0\n",
      "---train loss ---\n",
      "0.2624708414077759\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0014971767086535692\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001778426463715732\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009646265767514706\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0023797317408025265\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0021554194390773773\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007870609406381845\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004228961654007435\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016176927601918578\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003215424483641982\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0026231459341943264\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009396066889166832\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.008047491312026978\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.006276023108512163\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0059805978089571\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005554273258894682\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0011495694052428007\n",
      "1.0\n",
      "---train loss ---\n",
      "0.013607910834252834\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00629808334633708\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000761088915169239\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005150556564331055\n",
      "1.0\n",
      "---train loss ---\n",
      "0.06794644147157669\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0012626444222405553\n",
      "1.0\n",
      "---train loss ---\n",
      "0.022046059370040894\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.009079959243535995\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0065015465952456\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.009513523429632187\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.001858884934335947\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006394388619810343\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0014144637389108539\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.012862777337431908\n",
      "0.966796875\n",
      "---eval loss ---\n",
      "15.397298812866211\n",
      "0.001953125\n",
      "---eval loss ---\n",
      "15.037489891052246\n",
      "0.0009765625\n",
      " --- 12 ---\n",
      "---train loss ---\n",
      "0.04914889112114906\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007282206788659096\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003117765998467803\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009395020315423608\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0020287882070988417\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0020168123301118612\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004650161601603031\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00035045816912315786\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003953517880290747\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0005753411678597331\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0029124526772648096\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002149347448721528\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004945759195834398\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002007597591727972\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001282291254028678\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010712489020079374\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0030038647819310427\n",
      "1.0\n",
      "---train loss ---\n",
      "0.014264225028455257\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0003161535714752972\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004503028467297554\n",
      "1.0\n",
      "---train loss ---\n",
      "0.029084814712405205\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0023726201616227627\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004313442390412092\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0039046769961714745\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0024678825866431\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005137279629707336\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0013472322607412934\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015374957583844662\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001116328057833016\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01306999009102583\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007049553096294403\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007443935028277338\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.009348266758024693\n",
      "0.9716796875\n",
      "---eval loss ---\n",
      "14.313953399658203\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "14.054189682006836\n",
      "0.001953125\n",
      " --- 13 ---\n",
      "---train loss ---\n",
      "0.00612255884334445\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.01367820892482996\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.030567465350031853\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0015626330859959126\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001187759917229414\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005254608578979969\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009575455915182829\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007354662287980318\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.006539131980389357\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.01149755995720625\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.01882183365523815\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.014877966605126858\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.002528616925701499\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011541541665792465\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0023399016354233027\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004844021750614047\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00026063242694363\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004865728784352541\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0001444724330212921\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002250214893138036\n",
      "1.0\n",
      "---train loss ---\n",
      "0.013994270004332066\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0004282684822101146\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006905687972903252\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0021782061085104942\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009402565658092499\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000552869460079819\n",
      "1.0\n",
      "---train loss ---\n",
      "0.012584755197167397\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00045274317380972207\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0062240478582680225\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.007855910807847977\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0008932110504247248\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016086803225334734\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0031592396553605795\n",
      "0.9912109375\n",
      "---eval loss ---\n",
      "15.504405975341797\n",
      "0.001953125\n",
      "---eval loss ---\n",
      "15.068694114685059\n",
      "0.00390625\n",
      " --- 14 ---\n",
      "---train loss ---\n",
      "0.002039674436673522\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006063556647859514\n",
      "1.0\n",
      "---train loss ---\n",
      "0.020136628299951553\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.001100271474570036\n",
      "1.0\n",
      "---train loss ---\n",
      "0.008627906441688538\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0011069701286032796\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009118571761064231\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00012329618039075285\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007920715725049376\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003668754594400525\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005606975872069597\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0006875691469758749\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005486954469233751\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003940046299248934\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006533671403303742\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006160738412290812\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002553006401285529\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0025360581930726767\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0014123450964689255\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00013123608368914574\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0033942111767828465\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0014091178309172392\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0022099572233855724\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004240304697304964\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000608096132054925\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01716582477092743\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.013709352351725101\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0007800551247783005\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002578168176114559\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001315379049628973\n",
      "1.0\n",
      "---train loss ---\n",
      "0.010194131173193455\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0008687464869581163\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.021332306787371635\n",
      "0.962890625\n",
      "---eval loss ---\n",
      "16.977069854736328\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "16.415571212768555\n",
      "0.0\n",
      " --- 15 ---\n",
      "---train loss ---\n",
      "0.030356256291270256\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0019208501325920224\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006012831814587116\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0004721711156889796\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009216349571943283\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005691873840987682\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000897648511454463\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009159904439002275\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005468724411912262\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00130356103181839\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002794931351672858\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006868334603495896\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001846970641054213\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007993507897481322\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006769819185137749\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0005772334989160299\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00024514098186045885\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001055195927619934\n",
      "1.0\n",
      "---train loss ---\n",
      "0.014158875681459904\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0003364802978467196\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0013522044755518436\n",
      "1.0\n",
      "---train loss ---\n",
      "0.013235173188149929\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009226296679116786\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012230833526700735\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015391348861157894\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0014060137327760458\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005193084478378296\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0029309922829270363\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002016201615333557\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016697526443749666\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00037922756746411324\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0014452814357355237\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.002245643874630332\n",
      "0.9951171875\n",
      "---eval loss ---\n",
      "18.500417709350586\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "18.2376708984375\n",
      "0.0009765625\n",
      " --- 16 ---\n",
      "---train loss ---\n",
      "0.0005634170374833047\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001218824298121035\n",
      "1.0\n",
      "---train loss ---\n",
      "0.05785701051354408\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.00042990129441022873\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015494757099077106\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003160604042932391\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0015105982311069965\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007289526402018964\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0018206299282610416\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005991493817418814\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012819877592846751\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004777700640261173\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00041038033668883145\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00251815770752728\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002746913582086563\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010332140373066068\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0018915131222456694\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0030354061163961887\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002763284428510815\n",
      "1.0\n",
      "---train loss ---\n",
      "0.008764471858739853\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.027301406487822533\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.013092822395265102\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009725813288241625\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0011752566788345575\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003466328838840127\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012911747908219695\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0035646313335746527\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0021821868140250444\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001195243326947093\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002459155803080648\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00020602364384103566\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00025714049115777016\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.004195894114673138\n",
      "0.98828125\n",
      "---eval loss ---\n",
      "16.93012237548828\n",
      "0.0\n",
      "---eval loss ---\n",
      "16.532352447509766\n",
      "0.0\n",
      " --- 17 ---\n",
      "---train loss ---\n",
      "0.0007665619486942887\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010402254993095994\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003458933671936393\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.011950387619435787\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.007706655189394951\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.008706747554242611\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0012755945790559053\n",
      "1.0\n",
      "---train loss ---\n",
      "0.012964769266545773\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00036325454129837453\n",
      "1.0\n",
      "---train loss ---\n",
      "0.013669023290276527\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0013097967021167278\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0013251472264528275\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005783462896943092\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0028334136586636305\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010306062176823616\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003220941172912717\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005563683225773275\n",
      "1.0\n",
      "---train loss ---\n",
      "0.021161390468478203\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0010184720158576965\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007949350401759148\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00714081572368741\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00022528930276166648\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004074109252542257\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0008016476058401167\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00028199938242323697\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012621270725503564\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003133258782327175\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009239811450242996\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.056460022926330566\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005789229646325111\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0002311025164090097\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000462441414128989\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.003077297704294324\n",
      "0.98828125\n",
      "---eval loss ---\n",
      "16.284868240356445\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "15.689749717712402\n",
      "0.001953125\n",
      " --- 18 ---\n",
      "---train loss ---\n",
      "0.0003168417315464467\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004857804626226425\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005847544874995947\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007837312296032906\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01026055309921503\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.011779394932091236\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0011374446330592036\n",
      "1.0\n",
      "---train loss ---\n",
      "0.03346254676580429\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00032408078550361097\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001019483432173729\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007716454565525055\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005469066672958434\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005522771272808313\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0028758086264133453\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0013586207060143352\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001232094131410122\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0027583895716816187\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.024837177246809006\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.04612656682729721\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0003232457966078073\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019498385954648256\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007625730242580175\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0010833698324859142\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002600392559543252\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002059752121567726\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005107372999191284\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.010173880495131016\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0005102612776681781\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007086663972586393\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00150644953828305\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015821278793737292\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00031056461739353836\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.004165001679211855\n",
      "0.9892578125\n",
      "---eval loss ---\n",
      "14.773784637451172\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "14.151061058044434\n",
      "0.0009765625\n",
      " --- 19 ---\n",
      "---train loss ---\n",
      "0.008904770947992802\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0028494892176240683\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.027627671137452126\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.013678404502570629\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.005339806899428368\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0015520203160122037\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0008828162681311369\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019555503968149424\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006255053449422121\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005879417061805725\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009971522958949208\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004645682405680418\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005243437364697456\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00175135163590312\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016744210151955485\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005939769558608532\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0015159602044150233\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015384239377453923\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00221458962187171\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004659136466216296\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015952824614942074\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002888431481551379\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007972313091158867\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0005983270239084959\n",
      "1.0\n",
      "---train loss ---\n",
      "0.014772210270166397\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.008018174208700657\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005145745351910591\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006272481405176222\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003654313273727894\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006344899069517851\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009295355412177742\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005695447325706482\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.003508075140416622\n",
      "0.9931640625\n",
      "---eval loss ---\n",
      "14.002059936523438\n",
      "0.0\n",
      "---eval loss ---\n",
      "13.654205322265625\n",
      "0.0009765625\n",
      " --- 20 ---\n",
      "---train loss ---\n",
      "0.0007074365857988596\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002742040203884244\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009586033993400633\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005735672544687986\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000958806776907295\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00015131082909647375\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004908636212348938\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000420974480221048\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003757186059374362\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012461530277505517\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003794776275753975\n",
      "1.0\n",
      "---train loss ---\n",
      "0.008972205221652985\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00239909952506423\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00033012995845638216\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001082412782125175\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003787281457334757\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001894169399747625\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006171101122163236\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005918349488638341\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0013751634396612644\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0028832429088652134\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007221759879030287\n",
      "1.0\n",
      "---train loss ---\n",
      "0.014714214950799942\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000507546472363174\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005039789248257875\n",
      "1.0\n",
      "---train loss ---\n",
      "0.022545959800481796\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.0007601545657962561\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00027088678325526416\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010110967559739947\n",
      "1.0\n",
      "---train loss ---\n",
      "0.09782814979553223\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.004283225629478693\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0004675163945648819\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0018142431508749723\n",
      "0.994140625\n",
      "---eval loss ---\n",
      "14.724599838256836\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "14.188745498657227\n",
      "0.0\n",
      " --- 21 ---\n",
      "---train loss ---\n",
      "0.002248914912343025\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004564340226352215\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0006287534488365054\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000426814251113683\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005431617610156536\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0013558582868427038\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0020516400691121817\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0028041431214660406\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00015509252261836082\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00021553561964537948\n",
      "1.0\n",
      "---train loss ---\n",
      "8.586565672885627e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002047381130978465\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0021065841428935528\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0023944631684571505\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0013121877564117312\n",
      "1.0\n",
      "---train loss ---\n",
      "8.107499888865277e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0028618762735277414\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004496781155467033\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00046872158418409526\n",
      "1.0\n",
      "---train loss ---\n",
      "4.6483193727908656e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003498790320008993\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005431798752397299\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0037177412305027246\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00252091814763844\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.007227383088320494\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009383826982229948\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016988402931019664\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002200108952820301\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00022703333524987102\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0038000643253326416\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000374640803784132\n",
      "1.0\n",
      "---train loss ---\n",
      "9.144234354607761e-05\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.00570959597826004\n",
      "0.9921875\n",
      "---eval loss ---\n",
      "14.340160369873047\n",
      "0.0\n",
      "---eval loss ---\n",
      "13.689239501953125\n",
      "0.0\n",
      " --- 22 ---\n",
      "---train loss ---\n",
      "0.00043945881770923734\n",
      "1.0\n",
      "---train loss ---\n",
      "3.42689709214028e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.018320497125387192\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0031834582332521677\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.001434659119695425\n",
      "1.0\n",
      "---train loss ---\n",
      "0.016780970618128777\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0006032890523783863\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016320833237841725\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00067037099506706\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000621633545961231\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001342923380434513\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00027367298025637865\n",
      "1.0\n",
      "---train loss ---\n",
      "0.019352613016963005\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.004913211800158024\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00048405135748907924\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0014744505751878023\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003477783757261932\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0022565412800759077\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.02904524654150009\n",
      "0.90625\n",
      "---train loss ---\n",
      "0.021891828626394272\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00040554488077759743\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00012122435146011412\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000526782067026943\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0022392270620912313\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007759401341900229\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010412792908027768\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002005690708756447\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003510649548843503\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0038103379774838686\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0011850050650537014\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000737989554181695\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010629253229126334\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.002555073006078601\n",
      "0.9931640625\n",
      "---eval loss ---\n",
      "14.325597763061523\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "13.801255226135254\n",
      "0.0009765625\n",
      " --- 23 ---\n",
      "---train loss ---\n",
      "0.0008794805034995079\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004469452251214534\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0023436746560037136\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00045226002112030983\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00032592148636467755\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006214065942913294\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002127393236150965\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0038024343084543943\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001723212655633688\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0023020124062895775\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0002294249425176531\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004100982565432787\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002629403257742524\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0008600163855589926\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005774364341050386\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0005427324795164168\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019281470449641347\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00039745436515659094\n",
      "1.0\n",
      "---train loss ---\n",
      "8.000239176908508e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011803966481238604\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016816664719954133\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004387689696159214\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009158898610621691\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002427660161629319\n",
      "1.0\n",
      "---train loss ---\n",
      "6.277266220422462e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.030029576271772385\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00024941243464127183\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001188653870485723\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003762508276849985\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00862289872020483\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0008689662208780646\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000891079951543361\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.005929650738835335\n",
      "0.9853515625\n",
      "---eval loss ---\n",
      "13.515046119689941\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "13.536513328552246\n",
      "0.00390625\n",
      " --- 24 ---\n",
      "---train loss ---\n",
      "0.009143080562353134\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0010458905017003417\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003624880046118051\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0030907071195542812\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005558533011935651\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0008079500985331833\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010276816319674253\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000676759285852313\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006902109598740935\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00023644974862691015\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006306578405201435\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002490754472091794\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0006159857148304582\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006548247765749693\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0008583100279793143\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009051850065588951\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009381493437103927\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005673805717378855\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002203367417678237\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003966174554079771\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00023952416086103767\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00026624806923791766\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004448361578397453\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011195763945579529\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0006295238854363561\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007393727428279817\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005162911838851869\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002824815921485424\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0015986741054803133\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0032790005207061768\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002958140568807721\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016298441914841533\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.003578963689506054\n",
      "0.9912109375\n",
      "---eval loss ---\n",
      "12.059741020202637\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "11.757927894592285\n",
      "0.0029296875\n",
      " --- 25 ---\n",
      "---train loss ---\n",
      "0.0032853172160685062\n",
      "0.96875\n",
      "---train loss ---\n",
      "9.781253174878657e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016059719491750002\n",
      "1.0\n",
      "---train loss ---\n",
      "0.010370084084570408\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00030673190485686064\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000368004955817014\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004960215883329511\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0018998831510543823\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000613969808910042\n",
      "1.0\n",
      "---train loss ---\n",
      "0.011712351813912392\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002201922470703721\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009116638684645295\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00888899341225624\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00679624080657959\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.02044127881526947\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0036491146311163902\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.022418124601244926\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007277653203345835\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00022008623636793345\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001893882523290813\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00024323131947312504\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007057602051645517\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005485410802066326\n",
      "1.0\n",
      "---train loss ---\n",
      "5.488757597049698e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00020611546642612666\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003325958736240864\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00023679310106672347\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005710625555366278\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016206981672439724\n",
      "1.0\n",
      "---train loss ---\n",
      "0.021258391439914703\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00036244525108486414\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00015237045590765774\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0037093220744282007\n",
      "0.9814453125\n",
      "---eval loss ---\n",
      "13.271560668945312\n",
      "0.0\n",
      "---eval loss ---\n",
      "12.984335899353027\n",
      "0.0\n",
      " --- 26 ---\n",
      "---train loss ---\n",
      "0.0022780736908316612\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000201075614313595\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015551582910120487\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00019867649825755507\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00042721006320789456\n",
      "1.0\n",
      "---train loss ---\n",
      "3.207342160749249e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006902042077854276\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0015569556271657348\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003652101440820843\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001950224512256682\n",
      "1.0\n",
      "---train loss ---\n",
      "0.010118380188941956\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0005666546057909727\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005002639372833073\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005606242921203375\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016514949966222048\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0023466169368475676\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002110507106408477\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0032201185822486877\n",
      "1.0\n",
      "---train loss ---\n",
      "6.319524982245639e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003634077438618988\n",
      "1.0\n",
      "---train loss ---\n",
      "7.020519842626527e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.02945956587791443\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0010982082458212972\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005370098515413702\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009930740343406796\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001771989045664668\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007669847691431642\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001609417493455112\n",
      "1.0\n",
      "---train loss ---\n",
      "7.774157711537555e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005785717163234949\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0003470434166956693\n",
      "1.0\n",
      "---train loss ---\n",
      "0.024663345888257027\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.006910188589245081\n",
      "0.986328125\n",
      "---eval loss ---\n",
      "13.682869911193848\n",
      "0.0\n",
      "---eval loss ---\n",
      "13.424640655517578\n",
      "0.0\n",
      " --- 27 ---\n",
      "---train loss ---\n",
      "0.0024444630835205317\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001289783394895494\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002804691204801202\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01072245929390192\n",
      "0.96875\n",
      "---train loss ---\n",
      "9.203481022268534e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012619775952771306\n",
      "1.0\n",
      "---train loss ---\n",
      "0.10078706592321396\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.003825191641226411\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0013353473041206598\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005904277786612511\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0016828961670398712\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006024049129337072\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.006632536184042692\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.001924438402056694\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007331024389714003\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003314147179480642\n",
      "1.0\n",
      "---train loss ---\n",
      "0.019918235018849373\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.001755263190716505\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00027139330632053316\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00025242232368327677\n",
      "1.0\n",
      "---train loss ---\n",
      "0.019760537892580032\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0031037821900099516\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0002184223267249763\n",
      "1.0\n",
      "---train loss ---\n",
      "0.012279353104531765\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00018769061716739088\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009909386746585369\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.000218772140215151\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00027062708977609873\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002395135350525379\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0031738863326609135\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.009147679433226585\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00241092243231833\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.005972862243652344\n",
      "0.978515625\n",
      "---eval loss ---\n",
      "13.142159461975098\n",
      "0.0\n",
      "---eval loss ---\n",
      "12.978545188903809\n",
      "0.0\n",
      " --- 28 ---\n",
      "---train loss ---\n",
      "0.013477987609803677\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00017619306163396686\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0029454699251800776\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00117826322093606\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005904264398850501\n",
      "1.0\n",
      "---train loss ---\n",
      "5.6519536883570254e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005184076260775328\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00017343820945825428\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00021795094653498381\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005557647091336548\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006970980321057141\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0021896215621382\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00032278758590109646\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006358242244459689\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006067330250516534\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004365939530543983\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005793930497020483\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00015357632946688682\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005688872770406306\n",
      "1.0\n",
      "---train loss ---\n",
      "4.1780745959840715e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001553152920678258\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00014503327838610858\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006077899015508592\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019476806046441197\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004063993226736784\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00027349850279279053\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004010702483355999\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00031775765819475055\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0011025798739865422\n",
      "1.0\n",
      "---train loss ---\n",
      "0.018813123926520348\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0003449089708738029\n",
      "1.0\n",
      "---train loss ---\n",
      "5.7245095376856625e-05\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0019191242754459381\n",
      "0.9921875\n",
      "---eval loss ---\n",
      "11.488382339477539\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "11.272286415100098\n",
      "0.0\n",
      " --- 29 ---\n",
      "---train loss ---\n",
      "0.010511476546525955\n",
      "0.96875\n",
      "---train loss ---\n",
      "6.333820056170225e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006045405752956867\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00010348131763748825\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011256053403485566\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00494332006201148\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0002476355293765664\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006227193865925074\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0008823959506116807\n",
      "1.0\n",
      "---train loss ---\n",
      "0.010303555987775326\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009417528053745627\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002467934391461313\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001342723990092054\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0032548047602176666\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0051738191395998\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0001431936543667689\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007767537608742714\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002515154890716076\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006165212020277977\n",
      "1.0\n",
      "---train loss ---\n",
      "6.489657243946567e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "7.559624646091834e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.001370677025988698\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001809645036701113\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0023764187935739756\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0017975274240598083\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006780876312404871\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.006209082435816526\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0064773960039019585\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0007731533260084689\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003670334117487073\n",
      "1.0\n",
      "---train loss ---\n",
      "0.006985348183661699\n",
      "0.96875\n",
      "---train loss ---\n",
      "9.518806473352015e-05\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.00038706744089722633\n",
      "0.9990234375\n",
      "---eval loss ---\n",
      "13.471212387084961\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "13.090906143188477\n",
      "0.001953125\n",
      " --- 30 ---\n",
      "---train loss ---\n",
      "0.0001716772239888087\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004325042944401503\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0475347600877285\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.002765883458778262\n",
      "1.0\n",
      "---train loss ---\n",
      "4.700857971329242e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005497176316566765\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00019844113558065146\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0029415725730359554\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00023407467233482748\n",
      "1.0\n",
      "---train loss ---\n",
      "9.825145389186218e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00037785229505971074\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00378340482711792\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0016632252372801304\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009110940154641867\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00031978779588826\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000473020103527233\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006465050973929465\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006791804335080087\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0022452091798186302\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00011464157432783395\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007606791332364082\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0027732907328754663\n",
      "0.96875\n",
      "---train loss ---\n",
      "6.288050644798204e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010200896067544818\n",
      "1.0\n",
      "---train loss ---\n",
      "1.8163063941756263e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00030497286934405565\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019786597695201635\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00027382877306081355\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001169640599982813\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0017555562080815434\n",
      "1.0\n",
      "---train loss ---\n",
      "3.1369221687782556e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00254706759005785\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.0016491736751049757\n",
      "0.994140625\n",
      "---eval loss ---\n",
      "12.210464477539062\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "12.091635704040527\n",
      "0.0\n",
      " --- 31 ---\n",
      "---train loss ---\n",
      "0.00019657170923892409\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001377739681629464\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004346425179392099\n",
      "0.96875\n",
      "---train loss ---\n",
      "9.840657003223896e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005900514661334455\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00013995369954500347\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019045955268666148\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016795033297967166\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010530565632507205\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00032376343733631074\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011176222324138507\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016816497372929007\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00010536726767895743\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002725928497966379\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00022504669323097914\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00013042216596659273\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0013223382411524653\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005244601052254438\n",
      "1.0\n",
      "---train loss ---\n",
      "3.3568107028258964e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000310351955704391\n",
      "1.0\n",
      "---train loss ---\n",
      "0.11749360710382462\n",
      "0.875\n",
      "---train loss ---\n",
      "0.008638938888907433\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00013911869609728456\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00024436114472337067\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012211853172630072\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001979596127057448\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01004331186413765\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00047786079812794924\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005025513819418848\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004849146935157478\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0031443489715456963\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0002495227090548724\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.001698251930065453\n",
      "0.99609375\n",
      "---eval loss ---\n",
      "11.039902687072754\n",
      "0.0\n",
      "---eval loss ---\n",
      "11.09310245513916\n",
      "0.0\n",
      " --- 32 ---\n",
      "---train loss ---\n",
      "0.0006324813002720475\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0008591387304477394\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00038364811916835606\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016872951528057456\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011687501682899892\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016042693459894508\n",
      "1.0\n",
      "---train loss ---\n",
      "0.018576880916953087\n",
      "0.875\n",
      "---train loss ---\n",
      "0.00016043354116845876\n",
      "1.0\n",
      "---train loss ---\n",
      "0.04484836757183075\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0001193652642541565\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004369332920759916\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.008816933259367943\n",
      "0.96875\n",
      "---train loss ---\n",
      "7.630347681697458e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00014552270295098424\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00018282792007084936\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004976036143489182\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00029068515868857503\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016799679724499583\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001939382345881313\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00037808166234754026\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0011664902558550239\n",
      "1.0\n",
      "---train loss ---\n",
      "0.005443241912871599\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.003330706851556897\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006746110157109797\n",
      "1.0\n",
      "---train loss ---\n",
      "8.733084541745484e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "5.251072798273526e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00017315975856035948\n",
      "1.0\n",
      "---train loss ---\n",
      "0.014761557802557945\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.008481739088892937\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005758402869105339\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009986209915950894\n",
      "1.0\n",
      "---train loss ---\n",
      "4.6139695768943056e-05\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0009295256459154189\n",
      "0.9970703125\n",
      "---eval loss ---\n",
      "11.896821022033691\n",
      "0.0\n",
      "---eval loss ---\n",
      "11.885464668273926\n",
      "0.0\n",
      " --- 33 ---\n",
      "---train loss ---\n",
      "0.002819629153236747\n",
      "0.96875\n",
      "---train loss ---\n",
      "4.4401596824172884e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009528692462481558\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00034034214331768453\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005105566815473139\n",
      "1.0\n",
      "---train loss ---\n",
      "8.18749686004594e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002088721375912428\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0020530223846435547\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009936660062521696\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00025123567320406437\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0021085692569613457\n",
      "1.0\n",
      "---train loss ---\n",
      "7.44441567803733e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011374635505490005\n",
      "1.0\n",
      "---train loss ---\n",
      "0.020365403965115547\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.009558912366628647\n",
      "0.96875\n",
      "---train loss ---\n",
      "8.603982132626697e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004003951617050916\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00040718400850892067\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006945911445654929\n",
      "1.0\n",
      "---train loss ---\n",
      "8.81526866578497e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001953348401002586\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00017651327652856708\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004354233096819371\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003098415327258408\n",
      "1.0\n",
      "---train loss ---\n",
      "6.214138556970283e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000268688949290663\n",
      "1.0\n",
      "---train loss ---\n",
      "5.971129212412052e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002331104828044772\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00010400921746622771\n",
      "1.0\n",
      "---train loss ---\n",
      "8.163405436789617e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00023332318232860416\n",
      "1.0\n",
      "---train loss ---\n",
      "0.008038504980504513\n",
      "0.96875\n",
      "---eval loss ---\n",
      "0.0017947456799447536\n",
      "0.994140625\n",
      "---eval loss ---\n",
      "11.439952850341797\n",
      "0.0009765625\n",
      "---eval loss ---\n",
      "11.198906898498535\n",
      "0.0\n",
      " --- 34 ---\n",
      "---train loss ---\n",
      "4.18886193074286e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001131979952333495\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005304188234731555\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0017173145897686481\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001183711938210763\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00012463728489819914\n",
      "1.0\n",
      "---train loss ---\n",
      "5.248343586572446e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.003805882530286908\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00010491457942407578\n",
      "1.0\n",
      "---train loss ---\n",
      "5.857376163476147e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0011563572334125638\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011965762678300962\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0007260640850290656\n",
      "1.0\n",
      "---train loss ---\n",
      "8.514180808560923e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.016409488394856453\n",
      "0.96875\n",
      "---train loss ---\n",
      "8.761420758673921e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00021078581630717963\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00017640343867242336\n",
      "1.0\n",
      "---train loss ---\n",
      "9.694546315586194e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "8.412282477365807e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "4.1482693632133305e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01879139058291912\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0010985712287947536\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00031917935120873153\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005531428032554686\n",
      "1.0\n",
      "---train loss ---\n",
      "0.02335299365222454\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.0019487574463710189\n",
      "1.0\n",
      "---train loss ---\n",
      "5.045622674515471e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00019394046103116125\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001613334025023505\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00020904811390209943\n",
      "1.0\n",
      "---train loss ---\n",
      "3.227153865736909e-05\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0028079214971512556\n",
      "0.98828125\n",
      "---eval loss ---\n",
      "12.424642562866211\n",
      "0.0\n",
      "---eval loss ---\n",
      "12.373259544372559\n",
      "0.0\n",
      " --- 35 ---\n",
      "---train loss ---\n",
      "0.0001698314445093274\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011388147686375305\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006028172210790217\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0027815683279186487\n",
      "1.0\n",
      "---train loss ---\n",
      "7.16602080501616e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "2.502542338334024e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.008632934652268887\n",
      "0.9375\n",
      "---train loss ---\n",
      "4.06349245167803e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001744013134157285\n",
      "1.0\n",
      "---train loss ---\n",
      "5.4158721468411386e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "6.400471465894952e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0036150517407804728\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0004908600240014493\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0017876531928777695\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004850737750530243\n",
      "0.96875\n",
      "---train loss ---\n",
      "3.418828418944031e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006045403424650431\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000964139006100595\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00016396892897319049\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002523995644878596\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0018620595801621675\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000358529738150537\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002407892607152462\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00864094216376543\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0012571595143526793\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002927876776084304\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005242739571258426\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0022058452013880014\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.005112987011671066\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0006728814332745969\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007114115636795759\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00012545821664389223\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.000360549078322947\n",
      "0.998046875\n",
      "---eval loss ---\n",
      "11.292850494384766\n",
      "0.0\n",
      "---eval loss ---\n",
      "11.000528335571289\n",
      "0.0009765625\n",
      " --- 36 ---\n",
      "---train loss ---\n",
      "0.00011544147855602205\n",
      "1.0\n",
      "---train loss ---\n",
      "4.8312962462659925e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0004711214278358966\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0003415826358832419\n",
      "1.0\n",
      "---train loss ---\n",
      "6.296125502558425e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.009603033773601055\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.00011645083577604964\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006194143206812441\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0019983986858278513\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0012454227544367313\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0026602090802043676\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.0009017983684316278\n",
      "1.0\n",
      "---train loss ---\n",
      "0.002703312085941434\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010740401921793818\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0006869990611448884\n",
      "1.0\n",
      "---train loss ---\n",
      "3.7238150980556384e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01164089236408472\n",
      "0.96875\n",
      "---train loss ---\n",
      "7.234057557070628e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0010990124428644776\n",
      "1.0\n",
      "---train loss ---\n",
      "6.16808029008098e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00022337617701850832\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0001570119202369824\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007394261192530394\n",
      "0.96875\n",
      "---train loss ---\n",
      "5.329919804353267e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004275826271623373\n",
      "0.96875\n",
      "---train loss ---\n",
      "5.873867849004455e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.007811741437762976\n",
      "0.96875\n",
      "---train loss ---\n",
      "0.004190284293144941\n",
      "0.96875\n",
      "---train loss ---\n",
      "6.43279345240444e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0018840350676327944\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00044317430001683533\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0005474233767017722\n",
      "1.0\n",
      "---eval loss ---\n",
      "0.0004974379553459585\n",
      "0.9990234375\n",
      "---eval loss ---\n",
      "10.877037048339844\n",
      "0.0\n",
      "---eval loss ---\n",
      "10.792457580566406\n",
      "0.0009765625\n",
      " --- 37 ---\n",
      "---train loss ---\n",
      "0.00019540470384526998\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00017370433488395065\n",
      "1.0\n",
      "---train loss ---\n",
      "0.01911819539964199\n",
      "0.9375\n",
      "---train loss ---\n",
      "0.00028351263608783484\n",
      "1.0\n",
      "---train loss ---\n",
      "3.995606311946176e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00010945026588160545\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0002791839651763439\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0043634939938783646\n",
      "0.96875\n",
      "---train loss ---\n",
      "4.3862106394954026e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00011929846368730068\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000718168041203171\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0008014832274056971\n",
      "1.0\n",
      "---train loss ---\n",
      "0.0009874188108369708\n",
      "1.0\n",
      "---train loss ---\n",
      "0.000891726987902075\n",
      "1.0\n",
      "---train loss ---\n",
      "6.089596718084067e-05\n",
      "1.0\n",
      "---train loss ---\n",
      "0.00017332460265606642\n",
      "1.0\n",
      "---train loss ---\n",
      "0.004330738913267851\n",
      "0.96875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba_models_generative/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     16\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 248\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    245\u001b[0m data_tl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])\n\u001b[1;32m    247\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 248\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    251\u001b[0m s \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(data, loader, model, optimizer, device, status)\u001b[0m\n\u001b[1;32m    116\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 118\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    121\u001b[0m     w2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/optim/adamw.py:173\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    170\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     adamw(\n\u001b[1;32m    185\u001b[0m         params_with_grad,\n\u001b[1;32m    186\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/optim/adamw.py:109\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdamW does not support sparse gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m grads\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m--> 109\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# State initialization\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# note(crcrpar): Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/_tensor.py:1002\u001b[0m, in \u001b[0;36mTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    992\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    999\u001b[0m         )\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;66;03m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# subclass if needed.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--split\", default=0.5, type=float)\n",
    "parser.add_argument(\"--weight_decay\", default=0.01, type=float)\n",
    "parser.add_argument(\"--num_layers\", default=6, type=int)\n",
    "parser.add_argument(\"--d_model\", default=64, type=int)\n",
    "parser.add_argument(\"--epochs\",default=100, type=int)\n",
    "parser.add_argument(\"--batch_size\",default=32, type=int)\n",
    "parser.add_argument(\"--batch_size_eval\",default=1024, type=int)\n",
    "parser.add_argument(\"--learning_rate\",default=3e-3, type=float)\n",
    "parser.add_argument(\"--ID\", default=3, type=int)\n",
    "parser.add_argument(\"--ndigits\", default=5, type=int)\n",
    "parser.add_argument(\"--nextra\", default=5, type=int)\n",
    "parser.add_argument(\"--output_dir\", default=\"mamba_models_generative/\", type=str)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data, target, data_f, target_f = GenerateDataset(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel(128, 2, 12, device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(data[:10].to('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 12])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits[:, -6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'mamba_models/model_n2_s0.3_w0.2_0'\n",
    "n_layer = 2 # number of layers\n",
    "d_model = 128 # model dimension, residual stream\n",
    "vocab = 12\n",
    "\n",
    "model = MambaLMHeadModel(d_model = d_model, n_layer=n_layer, vocab_size=vocab, device='cuda:1')\n",
    "\n",
    "model.load_state_dict(torch.load(output_file)['model'])\n",
    "\n",
    "model = model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embed = model.backbone.embedding.weight.detach().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svdEMB = torch.svd(Embed)\n",
    "\n",
    "a, b = 1, 2\n",
    "\n",
    "x = svdEMB[0][:, a] * svdEMB[1][a]\n",
    "y = svdEMB[0][:, b] * svdEMB[1][b]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "i = 0\n",
    "for x, y in zip(x, y):\n",
    "    plt.annotate(f'{i}', (x, y))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaLMHeadModel(\n",
       "  (backbone): MixerModel(\n",
       "    (embedding): Embedding(12, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (mixer): Mamba(\n",
       "          (in_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "          (act): SiLU()\n",
       "          (x_proj): Linear(in_features=256, out_features=40, bias=False)\n",
       "          (dt_proj): Linear(in_features=8, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=12, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1907542f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = model.backbone.layers[0].mixer.x_proj.weight.detach().to('cpu')\n",
    "plt.imshow(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
