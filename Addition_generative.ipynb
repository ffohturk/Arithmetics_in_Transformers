{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "from requests import get\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDataset_pp(ndig, nextra, pad):\n",
    "\n",
    "    stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11,'x': 12}\n",
    "\n",
    "    data = []\n",
    "    target = []\n",
    "    data_f = []\n",
    "    target_f = []\n",
    "\n",
    "    k = 0\n",
    "    while k < 200000:\n",
    "        i = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "        j = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        # target_p.append(i+j)\n",
    "\n",
    "        if len(li) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(li), size=(1,))\n",
    "            r1 = ndig+nextra - len(li) - r0\n",
    "            li = ['x'] * r0 + li + ['x'] * r1\n",
    "        if len(lj) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(lj), size=(1,))\n",
    "            r1 = ndig+nextra - len(lj) - r0\n",
    "            lj = ['x'] * r0 + lj + ['x'] * r1\n",
    "        if len(lij) < ndig+nextra+pad:\n",
    "            r0 = torch.randint(0, ndig+nextra+pad - len(lij), size=(1,))\n",
    "            r1 = ndig+nextra+pad - len(lij) - r0\n",
    "            lij = ['x'] * r0 + lij + ['x'] * r1\n",
    "        \n",
    "        lsum = li + ['+'] + lj + lij\n",
    "        lt = lsum[1:] + ['=']\n",
    "        data.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "\n",
    "        include = False\n",
    "        while not include:\n",
    "            i = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            j = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            include = (i + j < 10**(ndig+nextra))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        # target_fp.append(i+j)\n",
    "\n",
    "        if len(li) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(li), size=(1,))\n",
    "            r1 = ndig+nextra - len(li) - r0\n",
    "            li = ['x'] * r0 + li + ['x'] * r1\n",
    "        if len(lj) < ndig+nextra:\n",
    "            r0 = torch.randint(0, ndig+nextra - len(lj), size=(1,))\n",
    "            r1 = ndig+nextra - len(lj) - r0\n",
    "            lj = ['x'] * r0 + lj + ['x'] * r1\n",
    "        if len(lij) < ndig+nextra+pad:\n",
    "            r0 = torch.randint(0, ndig+nextra+pad - len(lij), size=(1,))\n",
    "            r1 = ndig+nextra+pad - len(lij) - r0\n",
    "            lij = ['x'] * r0 + lij + ['x'] * r1\n",
    "        \n",
    "        lsum = li + ['+'] + lj + lij\n",
    "        lt = lsum[1:] + ['=']\n",
    "\n",
    "        data_f.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target_f.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "        k += 1\n",
    "\n",
    "    data_f = torch.LongTensor(data_f)\n",
    "    target_f = torch.LongTensor(target_f)\n",
    "    data = torch.LongTensor(data)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    vocab = len(stoi)\n",
    "    \n",
    "    return vocab, data, target, data_f, target_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTot(nn.Module):\n",
    "\n",
    "  def __init__(self, decoder, embed, generator):\n",
    "    super().__init__()\n",
    "    self.embed = embed\n",
    "    self.gen = generator\n",
    "    self.decoder = decoder\n",
    "    self.generator = generator\n",
    "\n",
    "  def forward(self, src, mask):\n",
    "    return self.generator(self.decoder(self.embed(src), mask))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, vocab_size):\n",
    "    super().__init__()\n",
    "    self.ln = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return F.log_softmax(self.ln(x), dim=-1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, attn, ffn, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        x1 = self.norm(x)\n",
    "        x = x + self.dropout(self.attn(x1, x1, x1, mask))\n",
    "        self.out = x + self.dropout(self.ffn(self.norm(x)))\n",
    "        return self.out\n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.norm = nn.LayerNorm(layer.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "def Attention(q, k, v, theta, mask=None, dropout=None):\n",
    "\n",
    "            ### -- Softmax Attention with RoFormer -- ###\n",
    "\n",
    "            # q, k, v are dims (batch_size, # heads, seq_len, d_{k,v})\n",
    "\n",
    "            m = torch.arange(k.shape[-2]).view(k.shape[-2], 1).to(q.device)\n",
    "            t = torch.arange(k.shape[-1]).view(1, k.shape[-1])\n",
    "            t = torch.exp( - ( 2 * np.log(theta) / k.shape[-1] ) * torch.floor(t/2.) ).to(q.device)\n",
    "            r1 = torch.cos(m * t)\n",
    "            r2 = torch.sin(m * t)\n",
    "\n",
    "            K = torch.cat((q, k, v))\n",
    "\n",
    "            Kp = torch.einsum('ijkl, kl -> ijkl', K, r1)\n",
    "\n",
    "            L = torch.kron(torch.eye(k.shape[-1]//2), torch.Tensor([[0,-1],[1,0]])).to(q.device)\n",
    "            K = torch.einsum('ijkl, ml -> ijkm', K, L)\n",
    "\n",
    "            Kp += torch.einsum('ijkl, kl -> ijkl', K, r2)\n",
    "\n",
    "            Kp = Kp.view(-1, k.shape[0], k.shape[1], k.shape[2], k.shape[-1])\n",
    "\n",
    "            q, k, v = Kp[0], Kp[1], v # Kp[2]\n",
    "\n",
    "            A = torch.matmul(q, k.transpose(-2,-1)) * k.size(-1)**(-0.5)\n",
    "\n",
    "            if mask is not None:\n",
    "                A.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "            O = F.softmax(A, dim=-1)\n",
    "\n",
    "            if dropout is not None:\n",
    "                O = dropout(O)\n",
    "\n",
    "            return torch.matmul(O, v), O\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.attn = None\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.dropout =  nn.Dropout(p=dropout)\n",
    "        self.theta = 1000.0\n",
    "\n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        x = [l(z).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) for l, z in zip(self.linears, (query, keys, values))]\n",
    "\n",
    "        y, self.attn = Attention(x[0], x[1], x[2], self.theta, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linears[-1](y)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = None\n",
    "        self.out_p = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = self.relu(self.w1(x))\n",
    "        self.out_p = self.w2(self.dropout(self.out))\n",
    "        return self.w2(self.dropout(self.out))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.Emb = nn.Embedding(src_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Emb(x) * np.sqrt(self.d_model)\n",
    "\n",
    "def make_model(vocab, N = 6, d_model = 512, d_ff = 2048, h = 8, dropout = 0.1):\n",
    "\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    model = DecoderTot(DecoderStack(Decoder( c(attn), c(ffn), d_model, dropout), N),\n",
    "                           Embeddings(vocab, d_model),  Generator(d_model, vocab))\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1: # This is there to not initialize the biases\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    # print('# of parameters =', sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "\n",
    "    return model\n",
    "\n",
    "def GenerateDataset(ndig):\n",
    "\n",
    "    P = 10**ndig\n",
    "\n",
    "    data_add = []\n",
    "    target_add = []\n",
    "\n",
    "    stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11}\n",
    "\n",
    "    for i in range(P):\n",
    "        for j in range(P):\n",
    "            li = list(f'{i}')\n",
    "            lj = list(f'{j}')\n",
    "            lij = list(f'{i+j}')\n",
    "            if i + j < P:\n",
    "                if len(li) < ndig:\n",
    "                    li = ['0'] * (ndig - len(li)) + li\n",
    "                if len(lj) < ndig:\n",
    "                    lj = ['0'] * (ndig - len(lj)) + lj\n",
    "                if len(lij) < ndig:\n",
    "                    lij = ['0'] * (ndig - len(lij)) + lij\n",
    "\n",
    "                lsum = li + ['+'] + lj + lij\n",
    "                lt = lsum[1:] + ['=']\n",
    "                data_add.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "                target_add.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "\n",
    "    vocab = len(stoi)\n",
    "\n",
    "    data_f = torch.LongTensor(data_add)\n",
    "    target_f = torch.LongTensor(target_add)\n",
    "\n",
    "    return vocab, data_f, target_f\n",
    "\n",
    "def GenerateDataset_p(ndig, nextra):\n",
    "\n",
    "    stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11}\n",
    "\n",
    "    data = []\n",
    "    target = []\n",
    "    data_f = []\n",
    "    target_f = []\n",
    "\n",
    "    k = 0\n",
    "    while k < 200000:\n",
    "        i = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "        j = int(10**(ndig*torch.rand(size=(1,)).item()))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        if len(li) < ndig+nextra:\n",
    "            li = ['0'] * (ndig+nextra - len(li)) + li\n",
    "        if len(lj) < ndig+nextra:\n",
    "            lj = ['0'] * (ndig+nextra - len(lj)) + lj\n",
    "        if len(lij) < ndig+nextra:\n",
    "            lij = ['0'] * (ndig+nextra - len(lij)) + lij\n",
    "\n",
    "        lsum = li + ['+'] + lj + lij\n",
    "        lt = lsum[1:] + ['=']\n",
    "        data.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "\n",
    "        include = False\n",
    "        while not include:\n",
    "            i = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            j = int(10**(ndig+(nextra)*torch.rand(size=(1,)).item()))\n",
    "            include = (i + j < 10**(ndig+nextra))\n",
    "\n",
    "        li = list(f'{i}')\n",
    "        lj = list(f'{j}')\n",
    "        lij = list(f'{i+j}')\n",
    "        if len(li) < ndig+nextra:\n",
    "            li = ['0'] * (ndig+nextra - len(li)) + li\n",
    "        if len(lj) < ndig+nextra:\n",
    "            lj = ['0'] * (ndig+nextra - len(lj)) + lj\n",
    "        if len(lij) < ndig+nextra:\n",
    "            lij = ['0'] * (ndig+nextra - len(lij)) + lij\n",
    "\n",
    "        lsum = li + ['+'] + lj + lij\n",
    "        lt = lsum[1:] + ['=']\n",
    "        data_f.append([stoi[lsum[i]] for i in range(len(lsum))])\n",
    "        target_f.append([stoi[lt[i]] for i in range(len(lt))])\n",
    "        k += 1\n",
    "\n",
    "    data_f = torch.LongTensor(data_f)\n",
    "    target_f = torch.LongTensor(target_f)\n",
    "    data = torch.LongTensor(data)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    vocab = len(stoi)\n",
    "    \n",
    "    return vocab, data, target, data_f, target_f\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, target):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = self.inputs[index]\n",
    "        tgt = self.target[index]\n",
    "\n",
    "        return src, tgt\n",
    "\n",
    "def prepare(rank, world_size, data, target, batch_size, pin_memory=True, num_workers=0):\n",
    "\n",
    "    dataset = Dataset(data, target)\n",
    "    # sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "\n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def accuracy_calc(model, inputs, targets, gen_length, pad_token):\n",
    "\n",
    "    acc = []\n",
    "\n",
    "    max_gen = 0\n",
    "\n",
    "    while max_gen < gen_length:\n",
    "        seq_len = inputs.shape[-1]\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).to('cuda:0')\n",
    "\n",
    "        out = model(inputs, mask)[:, -1]\n",
    "        next_tok = torch.argmax(out, -1)\n",
    "\n",
    "        max_gen += 1\n",
    "        inputs = torch.cat((inputs, next_tok.unsqueeze(1)), 1)\n",
    "\n",
    "        if (max_gen == gen_length):\n",
    "            for i in range(inputs.shape[0]):\n",
    "                in_z = inputs[i, -max_gen:][inputs[i, -max_gen:] != pad_token]\n",
    "                target_z = targets[i, -max_gen:][targets[i, -max_gen:] != pad_token]\n",
    "                try:\n",
    "                    acc_z = (in_z == target_z).float().min().item()\n",
    "\n",
    "                    acc.append(acc_z)\n",
    "                except:\n",
    "                    continue\n",
    "    return sum(acc) / len(acc) if len(acc) > 0 else 0.0\n",
    "\n",
    "def run_epoch(data, loader, model, optimizer, device, status='train'):\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        src, tgt = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        seq_len = src.shape[-1]\n",
    "        \n",
    "        pad = 20\n",
    "\n",
    "        num_digits = (seq_len - 1 - pad) // 3 + pad\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
    "        logits = model.forward(src, mask)[:, -(num_digits + 1):]\n",
    "        tgt = tgt[:, -(num_digits + 1):]\n",
    "\n",
    "        kl_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = kl_loss(logits.transpose(-1, -2), tgt) # We want inputs to be (bs, vocab_size, seq len), so needed a transpose. Targets are (bs, seq len) with values in [0, vocab_size]\n",
    "\n",
    "        # a = (torch.argmax(logits.detach(), dim=-1) == tgt).float()\n",
    "\n",
    "        # acc_p = sum((torch.argmax(logits.detach(), dim=-1)[i] == tgt[i]).float().min() for i in range(len(tgt))) / len(tgt)\n",
    "\n",
    "        if status == 'train':\n",
    "            if i % 100 == 0:\n",
    "                # acc = accuracy_calc(src, num_digits+1, tgt, pad_token=12)\n",
    "\n",
    "                a = [loss.detach().item()]\n",
    "                print(f'---{status} loss ---')\n",
    "                print(loss.detach().item())\n",
    "                # print(acc)\n",
    "\n",
    "                pre_data = torch.tensor(a) # accuracy per token, correctness, loss\n",
    "                data = torch.cat((data, pre_data.unsqueeze(0)), 0)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if status == 'eval':\n",
    "            if i % 100 == 0:\n",
    "                acc = accuracy_calc(model, src[:, :(seq_len - num_digits)], tgt, num_digits + 1, pad_token=12)\n",
    "\n",
    "                a = [acc, loss.detach().item()]\n",
    "                w2 = sum((p.data**2).sum() for p in model.parameters()).clone().detach().to('cpu')\n",
    "                a.append(w2.item())\n",
    "                print(f'---{status} loss ---')\n",
    "                print(loss.detach().item())\n",
    "                print(acc)\n",
    "\n",
    "                pre_data = torch.tensor(a) # accuracy per token, correctness, loss\n",
    "                data = torch.cat((data, pre_data.unsqueeze(0)), 0)\n",
    "\n",
    "        del loss, tgt, src, logits\n",
    "        gc.collect\n",
    "\n",
    "    return data\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    ### Dataset\n",
    "\n",
    "    print('--- Generating data ---')\n",
    "    vocab, data, target, data_f, target_f = GenerateDataset_pp(ndig=args.ndigits, nextra = args.nextra, pad=args.pad)\n",
    "    print('--- Finished generating data ---')\n",
    "    # Three way split (training, test)\n",
    "    random.seed()\n",
    "    z = list(zip(data.tolist(), target.tolist()))\n",
    "    random.shuffle(z)\n",
    "\n",
    "    z1, z2 = zip(*z)\n",
    "    src_array_sh, tgt_array_sh = torch.LongTensor(list(z1)), torch.LongTensor(list(z2))\n",
    "\n",
    "    rank = torch.device('cuda:0')\n",
    "    world_size = 0\n",
    "\n",
    "    # Dataset parameters\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    batch_size_eval = args.batch_size_eval\n",
    "\n",
    "    vocab = vocab\n",
    "\n",
    "    split = args.split\n",
    "\n",
    "    n1 = int(split*len(src_array_sh))\n",
    "    n2 = 2*n1\n",
    "    # n1 = 0\n",
    "    # n0 = 100\n",
    "    priming_examples = 0\n",
    "\n",
    "    src_train, src_test = src_array_sh[:n1], src_array_sh[n1:n2]\n",
    "    tgt_train, tgt_test = tgt_array_sh[:n1], tgt_array_sh[n1:n2]\n",
    "    src_long, tgt_long = data_f[:priming_examples], target_f[:priming_examples]\n",
    "    src_test_long, tgt_test_long = data_f[priming_examples:], target_f[priming_examples:]\n",
    "\n",
    "    # src_train = torch.cat((src_train, src_long), 0)\n",
    "    # tgt_train = torch.cat((tgt_train, tgt_long), 0)\n",
    "    # src_train = src_long\n",
    "    # tgt_train = tgt_long\n",
    "\n",
    "    random.seed()\n",
    "    z = list(zip(src_train.tolist(), tgt_train.tolist()))\n",
    "    random.shuffle(z)\n",
    "\n",
    "    z1, z2 = zip(*z)\n",
    "    src_train, tgt_train = torch.LongTensor(list(z1)), torch.LongTensor(list(z2))\n",
    "\n",
    "    dataloader_train = prepare(rank, world_size, src_train, tgt_train, batch_size)\n",
    "\n",
    "    dataloader_test = prepare(rank, world_size, src_test, tgt_test, batch_size_eval)\n",
    "\n",
    "    dataloader_test_long = prepare(rank, world_size, src_test_long, tgt_test_long, batch_size_eval)\n",
    "\n",
    "    ### Model\n",
    "\n",
    "    d_model = args.d_model\n",
    "    d_ff = args.d_ff\n",
    "    n_heads = args.heads\n",
    "    n_layers = args.num_layers\n",
    "\n",
    "    # model = make_model(d_model=d_model,\n",
    "    #                    n_heads=n_heads,\n",
    "    #                    d_ff=d_ff,\n",
    "    #                    vocab=vocab,\n",
    "    #                    N=n_layers\n",
    "    #                    )\n",
    "\n",
    "    model = make_model(vocab, N = n_layers, d_model = d_model, d_ff = d_ff, h = n_heads, dropout = 0.1)\n",
    "\n",
    "    model = model.to(rank)\n",
    "\n",
    "    ### Training parameters and optimizer\n",
    "\n",
    "    lr = args.learning_rate\n",
    "    weight_decay = args.weight_decay    \n",
    "    num_epochs = args.epochs\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr = lr,\n",
    "                                  betas = (0.9, 0.98),\n",
    "                                  eps=1e-8,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "    ### Tracking\n",
    "\n",
    "    theta = model.decoder.layers[0].attn.theta\n",
    "\n",
    "    wandb.init(project=\"Generative Addition\",\n",
    "                     config={\"lr\": lr,\n",
    "                             \"split\":split,\n",
    "                             \"layers\": n_layers,\n",
    "                             \"weight decay\": weight_decay,\n",
    "                             \"d_ff\": d_ff,\n",
    "                             \"d_model\": d_model,\n",
    "                             \"heads\": n_heads,\n",
    "                             \"theta\": theta,\n",
    "                             \"Priming examples\": priming_examples,\n",
    "                             \"n digits train\": args.ndigits,\n",
    "                             \"n digits test\": f'{args.ndigits} + {args.nextra}',\n",
    "                             \"batch size\": batch_size,\n",
    "                             \"batch_size_eval\": batch_size_eval,\n",
    "                             'random padding': True,\n",
    "                             'ID': args.ID,\n",
    "                             'Comment': 'Extra padding'\n",
    "                             }\n",
    "                        )\n",
    "\n",
    "    ### Training    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f' --- {epoch} ---')\n",
    "        data = torch.tensor([])\n",
    "        data_t = torch.tensor([])\n",
    "        data_tl = torch.tensor([])\n",
    "\n",
    "        model.train()\n",
    "        data = run_epoch(data, loader=dataloader_train, model=model, optimizer=optimizer, device=rank, status='train')\n",
    "        \n",
    "        data = data.mean(dim=0)\n",
    "        s = {}\n",
    "        s['training loss'] = data[-1]\n",
    "        # z = [f'training acc. pos {i}' for i in range(args.ndigits + args.nextra + 1)]\n",
    "        # for i in range(len(z)):\n",
    "        #     s[z[i]] = data[i]\n",
    "        # s['training acc.'] = data[-2]\n",
    "        wandb.log(s,\n",
    "            step=epoch\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_t = run_epoch(data_t, loader=dataloader_test, model=model, optimizer=optimizer, device=rank, status='eval')\n",
    "\n",
    "            data_tl = run_epoch(data_tl, loader=dataloader_test_long, model=model, optimizer=optimizer, device=rank, status='eval')\n",
    "\n",
    "        data = data_t.mean(dim=0)\n",
    "        data_long = data_tl.mean(dim=0)\n",
    "\n",
    "        s = {}\n",
    "        s['test loss'] = data[-2]\n",
    "        # z = [f'test acc. pos {i}' for i in range(args.ndigits + args.nextra + 1)]\n",
    "        # for i in range(len(z)):\n",
    "        #     s[z[i]] = data[i]\n",
    "        s['test acc.'] = data[-3]\n",
    "        s['norm weights squared'] = data[-1]\n",
    "\n",
    "        s['test loss long'] = data_long[-2]\n",
    "        # z = [f'test acc. pos {i} long' for i in range(args.ndigits + args.nextra + 1)]\n",
    "        # for i in range(len(z)):\n",
    "        #     s[z[i]] = data_long[i]\n",
    "        s['test acc. long'] = data_long[-3]\n",
    "        wandb.log(s,\n",
    "            step=epoch\n",
    "            )\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            outputFile = args.output_dir + '/model_n{!s}_s{!s}_w{!s}_epoch{!s}'.format(n_layers, split, weight_decay, epoch)\n",
    "\n",
    "            torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                    }, outputFile)\n",
    "\n",
    "    # cleanup()\n",
    "    wandb.finish()\n",
    "\n",
    "    outputFile = args.output_dir + '/model_n{!s}_s{!s}_w{!s}_{!s}_final'.format(n_layers, split, weight_decay, args.ID)\n",
    "\n",
    "    torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }, outputFile)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating data ---\n",
      "--- Finished generating data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5fltbb9i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23f5bbdb4f046bdb2c63e9d972e6f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>norm weights squared</td><td>▁</td></tr><tr><td>test acc.</td><td>▁</td></tr><tr><td>test acc. long</td><td>▁</td></tr><tr><td>test loss</td><td>▁</td></tr><tr><td>test loss long</td><td>▁</td></tr><tr><td>training loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>norm weights squared</td><td>26330.5625</td></tr><tr><td>test acc.</td><td>0.0</td></tr><tr><td>test acc. long</td><td>0.0</td></tr><tr><td>test loss</td><td>0.32588</td></tr><tr><td>test loss long</td><td>1.50412</td></tr><tr><td>training loss</td><td>0.32457</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-field-92</strong> at: <a href='https://wandb.ai/iasai/Generative%20Addition/runs/5fltbb9i' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition/runs/5fltbb9i</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231221_085923-5fltbb9i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5fltbb9i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kruthoff/addition_project/wandb/run-20231221_090548-nfq8qdnw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iasai/Generative%20Addition/runs/nfq8qdnw' target=\"_blank\">dauntless-flower-93</a></strong> to <a href='https://wandb.ai/iasai/Generative%20Addition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iasai/Generative%20Addition' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iasai/Generative%20Addition/runs/nfq8qdnw' target=\"_blank\">https://wandb.ai/iasai/Generative%20Addition/runs/nfq8qdnw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- 0 ---\n",
      "---train loss ---\n",
      "2.3001492023468018\n",
      "---train loss ---\n",
      "0.3679562211036682\n",
      "---train loss ---\n",
      "0.33088219165802\n",
      "---train loss ---\n",
      "0.3423044979572296\n",
      "---train loss ---\n",
      "0.3427691161632538\n",
      "---train loss ---\n",
      "0.31426724791526794\n",
      "---train loss ---\n",
      "0.3301226794719696\n",
      "---train loss ---\n",
      "0.3248089849948883\n",
      "---train loss ---\n",
      "0.29598748683929443\n",
      "---train loss ---\n",
      "0.3071960210800171\n",
      "---train loss ---\n",
      "0.2810583710670471\n",
      "---train loss ---\n",
      "0.30911269783973694\n",
      "---train loss ---\n",
      "0.2952254116535187\n",
      "---train loss ---\n",
      "0.2798914909362793\n",
      "---train loss ---\n",
      "0.29760074615478516\n",
      "---train loss ---\n",
      "0.2711428999900818\n",
      "---train loss ---\n",
      "0.2715654969215393\n",
      "---train loss ---\n",
      "0.2732039988040924\n",
      "---train loss ---\n",
      "0.3008213937282562\n",
      "---eval loss ---\n",
      "0.2713318467140198\n",
      "0.0\n",
      "---eval loss ---\n",
      "0.2721307873725891\n",
      "0.0\n",
      "---eval loss ---\n",
      "0.2798798084259033\n",
      "0.0\n",
      "---eval loss ---\n",
      "0.283500611782074\n",
      "0.0\n",
      "---eval loss ---\n",
      "0.2872689366340637\n",
      "0.008547008547008548\n",
      "---eval loss ---\n",
      "1.5040411949157715\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5395063161849976\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.507778525352478\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5757429599761963\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.4843907356262207\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5158551931381226\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5052493810653687\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5055738687515259\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.4965537786483765\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5191879272460938\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.593973994255066\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5163240432739258\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.492113471031189\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5623390674591064\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.5123817920684814\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.493635892868042\n",
      "0.0\n",
      " --- 1 ---\n",
      "---train loss ---\n",
      "0.2632893919944763\n",
      "---train loss ---\n",
      "0.28966477513313293\n",
      "---train loss ---\n",
      "0.25543028116226196\n",
      "---train loss ---\n",
      "0.2757883667945862\n",
      "---train loss ---\n",
      "0.26923704147338867\n",
      "---train loss ---\n",
      "0.2551574110984802\n",
      "---train loss ---\n",
      "0.26539167761802673\n",
      "---train loss ---\n",
      "0.2558354139328003\n",
      "---train loss ---\n",
      "0.2493506371974945\n",
      "---train loss ---\n",
      "0.27592527866363525\n",
      "---train loss ---\n",
      "0.22716817259788513\n",
      "---train loss ---\n",
      "0.255991131067276\n",
      "---train loss ---\n",
      "0.24655942618846893\n",
      "---train loss ---\n",
      "0.2370387315750122\n",
      "---train loss ---\n",
      "0.24663510918617249\n",
      "---train loss ---\n",
      "0.2289820909500122\n",
      "---train loss ---\n",
      "0.22412192821502686\n",
      "---train loss ---\n",
      "0.22910326719284058\n",
      "---train loss ---\n",
      "0.24340060353279114\n",
      "---eval loss ---\n",
      "0.2220529466867447\n",
      "0.0975609756097561\n",
      "---eval loss ---\n",
      "0.2259005606174469\n",
      "0.11475409836065574\n",
      "---eval loss ---\n",
      "0.22718334197998047\n",
      "0.09523809523809523\n",
      "---eval loss ---\n",
      "0.24537484347820282\n",
      "0.04918032786885246\n",
      "---eval loss ---\n",
      "0.2453431785106659\n",
      "0.05737704918032787\n",
      "---eval loss ---\n",
      "1.7941118478775024\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.8180667161941528\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.8095561265945435\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.8401750326156616\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.762596845626831\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7860568761825562\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7789403200149536\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7684966325759888\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7708946466445923\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7875275611877441\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.8803969621658325\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.8087533712387085\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7790486812591553\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.824985384941101\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.7770905494689941\n",
      "0.0\n",
      "---eval loss ---\n",
      "1.8035361766815186\n",
      "0.0\n",
      " --- 2 ---\n",
      "---train loss ---\n",
      "0.23823298513889313\n",
      "---train loss ---\n",
      "0.24383926391601562\n",
      "---train loss ---\n",
      "0.20487840473651886\n",
      "---train loss ---\n",
      "0.22652535140514374\n",
      "---train loss ---\n",
      "0.219842791557312\n",
      "---train loss ---\n",
      "0.20926468074321747\n",
      "---train loss ---\n",
      "0.22912223637104034\n",
      "---train loss ---\n",
      "0.21422797441482544\n",
      "---train loss ---\n",
      "0.19922244548797607\n",
      "---train loss ---\n",
      "0.21003204584121704\n",
      "---train loss ---\n",
      "0.16548721492290497\n",
      "---train loss ---\n",
      "0.191999152302742\n",
      "---train loss ---\n",
      "0.16834858059883118\n",
      "---train loss ---\n",
      "0.19229832291603088\n",
      "---train loss ---\n",
      "0.1778235286474228\n",
      "---train loss ---\n",
      "0.15935178101062775\n",
      "---train loss ---\n",
      "0.17978502810001373\n",
      "---train loss ---\n",
      "0.15811707079410553\n",
      "---train loss ---\n",
      "0.1765529066324234\n",
      "---eval loss ---\n",
      "0.15413793921470642\n",
      "0.392\n",
      "---eval loss ---\n",
      "0.16152352094650269\n",
      "0.43089430894308944\n",
      "---eval loss ---\n",
      "0.15411201119422913\n",
      "0.4251968503937008\n",
      "---eval loss ---\n",
      "0.1712701916694641\n",
      "0.3225806451612903\n",
      "---eval loss ---\n",
      "0.17323631048202515\n",
      "0.32\n",
      "---eval loss ---\n",
      "2.1361939907073975\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.141601324081421\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1250994205474854\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.202549457550049\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.081454277038574\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1023342609405518\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.11751389503479\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.0951008796691895\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.11470890045166\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.157264232635498\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.257486581802368\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.147068738937378\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.0578956604003906\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2016987800598145\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1373143196105957\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1715822219848633\n",
      "0.0\n",
      " --- 3 ---\n",
      "---train loss ---\n",
      "0.1725197285413742\n",
      "---train loss ---\n",
      "0.17829450964927673\n",
      "---train loss ---\n",
      "0.13912324607372284\n",
      "---train loss ---\n",
      "0.1737518161535263\n",
      "---train loss ---\n",
      "0.15766523778438568\n",
      "---train loss ---\n",
      "0.15684874355793\n",
      "---train loss ---\n",
      "0.1539180874824524\n",
      "---train loss ---\n",
      "0.14512786269187927\n",
      "---train loss ---\n",
      "0.15801016986370087\n",
      "---train loss ---\n",
      "0.1628652662038803\n",
      "---train loss ---\n",
      "0.1422884464263916\n",
      "---train loss ---\n",
      "0.16372856497764587\n",
      "---train loss ---\n",
      "0.15017300844192505\n",
      "---train loss ---\n",
      "0.16784898936748505\n",
      "---train loss ---\n",
      "0.1596153974533081\n",
      "---train loss ---\n",
      "0.15245087444782257\n",
      "---train loss ---\n",
      "0.14974260330200195\n",
      "---train loss ---\n",
      "0.150455042719841\n",
      "---train loss ---\n",
      "0.17395853996276855\n",
      "---eval loss ---\n",
      "0.14239729940891266\n",
      "0.4838709677419355\n",
      "---eval loss ---\n",
      "0.14908739924430847\n",
      "0.44881889763779526\n",
      "---eval loss ---\n",
      "0.14245480298995972\n",
      "0.44881889763779526\n",
      "---eval loss ---\n",
      "0.15860778093338013\n",
      "0.3821138211382114\n",
      "---eval loss ---\n",
      "0.15908725559711456\n",
      "0.4032258064516129\n",
      "---eval loss ---\n",
      "2.2524280548095703\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2817890644073486\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.229883909225464\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.319392442703247\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2636377811431885\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.281184673309326\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2490320205688477\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.257127046585083\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.21354079246521\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2946884632110596\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.422506093978882\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.3307783603668213\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.227617025375366\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.332498550415039\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2712578773498535\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2496960163116455\n",
      "0.0\n",
      " --- 4 ---\n",
      "---train loss ---\n",
      "0.16728907823562622\n",
      "---train loss ---\n",
      "0.17152221500873566\n",
      "---train loss ---\n",
      "0.12807032465934753\n",
      "---train loss ---\n",
      "0.15322329103946686\n",
      "---train loss ---\n",
      "0.1498592644929886\n",
      "---train loss ---\n",
      "0.13677752017974854\n",
      "---train loss ---\n",
      "0.14880110323429108\n",
      "---train loss ---\n",
      "0.13615065813064575\n",
      "---train loss ---\n",
      "0.15533290803432465\n",
      "---train loss ---\n",
      "0.15431150794029236\n",
      "---train loss ---\n",
      "0.12531320750713348\n",
      "---train loss ---\n",
      "0.1483205407857895\n",
      "---train loss ---\n",
      "0.14560763537883759\n",
      "---train loss ---\n",
      "0.15030458569526672\n",
      "---train loss ---\n",
      "0.14559023082256317\n",
      "---train loss ---\n",
      "0.1399083286523819\n",
      "---train loss ---\n",
      "0.1450461894273758\n",
      "---train loss ---\n",
      "0.1372031271457672\n",
      "---train loss ---\n",
      "0.14935562014579773\n",
      "---eval loss ---\n",
      "0.13176658749580383\n",
      "0.576271186440678\n",
      "---eval loss ---\n",
      "0.13504895567893982\n",
      "0.5371900826446281\n",
      "---eval loss ---\n",
      "0.1320231407880783\n",
      "0.56\n",
      "---eval loss ---\n",
      "0.14452135562896729\n",
      "0.5086206896551724\n",
      "---eval loss ---\n",
      "0.14866045117378235\n",
      "0.4576271186440678\n",
      "---eval loss ---\n",
      "2.1985127925872803\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.253760576248169\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.113279104232788\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2459709644317627\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.187285900115967\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1635544300079346\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2099061012268066\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.227182626724243\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1393074989318848\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.222895622253418\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.358530282974243\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.26138973236084\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.119302272796631\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.224579095840454\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.216029405593872\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.190772533416748\n",
      "0.0\n",
      " --- 5 ---\n",
      "---train loss ---\n",
      "0.16096140444278717\n",
      "---train loss ---\n",
      "0.15112969279289246\n",
      "---train loss ---\n",
      "0.12010344117879868\n",
      "---train loss ---\n",
      "0.1414676457643509\n",
      "---train loss ---\n",
      "0.14523708820343018\n",
      "---train loss ---\n",
      "0.1376888006925583\n",
      "---train loss ---\n",
      "0.13931918144226074\n",
      "---train loss ---\n",
      "0.1270502507686615\n",
      "---train loss ---\n",
      "0.13783903419971466\n",
      "---train loss ---\n",
      "0.14504368603229523\n",
      "---train loss ---\n",
      "0.12600912153720856\n",
      "---train loss ---\n",
      "0.13762560486793518\n",
      "---train loss ---\n",
      "0.13587598502635956\n",
      "---train loss ---\n",
      "0.1364879161119461\n",
      "---train loss ---\n",
      "0.13273665308952332\n",
      "---train loss ---\n",
      "0.12860926985740662\n",
      "---train loss ---\n",
      "0.12666188180446625\n",
      "---train loss ---\n",
      "0.11651391535997391\n",
      "---train loss ---\n",
      "0.12853845953941345\n",
      "---eval loss ---\n",
      "0.11249545961618423\n",
      "0.746031746031746\n",
      "---eval loss ---\n",
      "0.118096262216568\n",
      "0.753968253968254\n",
      "---eval loss ---\n",
      "0.1171741858124733\n",
      "0.6875\n",
      "---eval loss ---\n",
      "0.12134894728660583\n",
      "0.6611570247933884\n",
      "---eval loss ---\n",
      "0.12364708632230759\n",
      "0.6190476190476191\n",
      "---eval loss ---\n",
      "2.0776822566986084\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.21889328956604\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1048309803009033\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.178769588470459\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.139554262161255\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1695339679718018\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1528713703155518\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.0889956951141357\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1098392009735107\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.1604490280151367\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.318303108215332\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2049431800842285\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.0724542140960693\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.168663501739502\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.175325632095337\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.2503929138183594\n",
      "0.0\n",
      " --- 6 ---\n",
      "---train loss ---\n",
      "0.13560982048511505\n",
      "---train loss ---\n",
      "0.1458352953195572\n",
      "---train loss ---\n",
      "0.10957158356904984\n",
      "---train loss ---\n",
      "0.1247059628367424\n",
      "---train loss ---\n",
      "0.12422297894954681\n",
      "---train loss ---\n",
      "0.1129850447177887\n",
      "---train loss ---\n",
      "0.12300638109445572\n",
      "---train loss ---\n",
      "0.10996011644601822\n",
      "---train loss ---\n",
      "0.1438281536102295\n",
      "---train loss ---\n",
      "0.1256103813648224\n",
      "---train loss ---\n",
      "0.11622076481580734\n",
      "---train loss ---\n",
      "0.12416060268878937\n",
      "---train loss ---\n",
      "0.12554848194122314\n",
      "---train loss ---\n",
      "0.1257534772157669\n",
      "---train loss ---\n",
      "0.12255380302667618\n",
      "---train loss ---\n",
      "0.12271404266357422\n",
      "---train loss ---\n",
      "0.13276119530200958\n",
      "---train loss ---\n",
      "0.12090004980564117\n",
      "---train loss ---\n",
      "0.10815417021512985\n",
      "---eval loss ---\n",
      "0.10109493136405945\n",
      "0.875\n",
      "---eval loss ---\n",
      "0.10248947143554688\n",
      "0.848\n",
      "---eval loss ---\n",
      "0.10074523836374283\n",
      "0.8359375\n",
      "---eval loss ---\n",
      "0.10705359280109406\n",
      "0.7619047619047619\n",
      "---eval loss ---\n",
      "0.10408005118370056\n",
      "0.7795275590551181\n",
      "---eval loss ---\n",
      "2.623237133026123\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.836840867996216\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.7467074394226074\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.9439101219177246\n",
      "0.0\n",
      "---eval loss ---\n",
      "2.7904176712036133\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     19\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 499\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    497\u001b[0m     data_t \u001b[38;5;241m=\u001b[39m run_epoch(data_t, loader\u001b[38;5;241m=\u001b[39mdataloader_test, model\u001b[38;5;241m=\u001b[39mmodel, optimizer\u001b[38;5;241m=\u001b[39moptimizer, device\u001b[38;5;241m=\u001b[39mrank, status\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 499\u001b[0m     data_tl \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_tl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_test_long\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m data \u001b[38;5;241m=\u001b[39m data_t\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    502\u001b[0m data_long \u001b[38;5;241m=\u001b[39m data_tl\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 318\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(data, loader, model, optimizer, device, status)\u001b[0m\n\u001b[1;32m    316\u001b[0m num_digits \u001b[38;5;241m=\u001b[39m (seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m pad) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m pad\n\u001b[1;32m    317\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones(seq_len, seq_len))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 318\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m(num_digits \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):]\n\u001b[1;32m    319\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m-\u001b[39m(num_digits \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):]\n\u001b[1;32m    321\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mDecoderTot.forward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, mask):\n\u001b[0;32m---> 11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36mDecoderStack.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 51\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     36\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)))\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 109\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, keys, values, mask)\u001b[0m\n\u001b[1;32m    105\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    107\u001b[0m x \u001b[38;5;241m=\u001b[39m [l(z)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l, z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears, (query, keys, values))]\n\u001b[0;32m--> 109\u001b[0m y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](y)\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mAttention\u001b[0;34m(q, k, v, theta, mask, dropout)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAttention\u001b[39m(q, k, v, theta, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m             \u001b[38;5;66;03m### -- Softmax Attention with RoFormer -- ###\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m             \u001b[38;5;66;03m# q, k, v are dims (batch_size, # heads, seq_len, d_{k,v})\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m             m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m             t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     62\u001b[0m             t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp( \u001b[38;5;241m-\u001b[39m ( \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(theta) \u001b[38;5;241m/\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] ) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloor(t\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.\u001b[39m) )\u001b[38;5;241m.\u001b[39mto(q\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--split\", default=0.3, type=float)\n",
    "parser.add_argument(\"--weight_decay\", default=0.3, type=float)\n",
    "parser.add_argument(\"--num_layers\", default=6, type=int)\n",
    "parser.add_argument(\"--d_model\", default=512, type=int)\n",
    "parser.add_argument(\"--d_ff\", default=2048, type=int)\n",
    "parser.add_argument(\"--heads\", default=8, type=int)\n",
    "parser.add_argument(\"--epochs\", default=10, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "parser.add_argument(\"--batch_size_eval\", default=128, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=5e-4, type=float)\n",
    "parser.add_argument(\"--ndigits\", default=5, type=int)\n",
    "parser.add_argument(\"--nextra\", default=15, type=int)\n",
    "parser.add_argument(\"--pad\", default=20, type=int)\n",
    "parser.add_argument(\"--output_dir\", default=\"generative/\", type=str)\n",
    "parser.add_argument(\"--ID\", default=120, type=int)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data, target, data_f, target_f = GenerateDataset_pp(5, 15, pad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'generative/model_n6_s0.3_w0.3_122_final'\n",
    "n_layer = 6 # number of layers\n",
    "d_model = 512 # model dimension, residual stream\n",
    "d_ff = 2048 # dim intermediate feed-forward layer\n",
    "h_a = 8 # number of heads in attention (doesnt impact # of params)\n",
    "\n",
    "model = make_model(vocab=13, N = n_layer, d_model = d_model, d_ff = d_ff, h = h_a)\n",
    "\n",
    "model.load_state_dict(torch.load(output_file)['model'])\n",
    "\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 6, 8, 9], device='cuda:0') tensor([1, 6, 8, 9], device='cuda:0')\n",
      "tensor([4, 7, 1, 9], device='cuda:0') tensor([4, 7, 1, 9], device='cuda:0')\n",
      "tensor([1, 4, 2, 5, 6], device='cuda:0') tensor([1, 4, 2, 5, 6], device='cuda:0')\n",
      "tensor([1, 6, 5, 1, 6], device='cuda:0') tensor([1, 6, 5, 1, 6], device='cuda:0')\n",
      "tensor([1, 6, 2, 2], device='cuda:0') tensor([1, 6, 2, 2], device='cuda:0')\n",
      "tensor([6, 5], device='cuda:0') tensor([6, 5], device='cuda:0')\n",
      "tensor([3, 8], device='cuda:0') tensor([3, 8], device='cuda:0')\n",
      "tensor([2, 3, 2], device='cuda:0') tensor([2, 3, 2], device='cuda:0')\n",
      "tensor([5, 4, 4, 3], device='cuda:0') tensor([5, 4, 4, 3], device='cuda:0')\n",
      "tensor([8, 5, 5, 0, 7], device='cuda:0') tensor([8, 5, 5, 0, 7], device='cuda:0')\n",
      "tensor([2, 6, 6, 1], device='cuda:0') tensor([2, 6, 6, 1], device='cuda:0')\n",
      "tensor([2, 2, 7, 6], device='cuda:0') tensor([2, 2, 7, 6], device='cuda:0')\n",
      "tensor([4, 7], device='cuda:0') tensor([4, 7], device='cuda:0')\n",
      "tensor([6, 0], device='cuda:0') tensor([6, 0], device='cuda:0')\n",
      "tensor([3, 7, 2, 3], device='cuda:0') tensor([3, 7, 2, 3], device='cuda:0')\n",
      "tensor([6, 1], device='cuda:0') tensor([6, 1], device='cuda:0')\n",
      "tensor([2, 3, 2, 5, 2], device='cuda:0') tensor([2, 3, 2, 5, 2], device='cuda:0')\n",
      "tensor([9, 3, 8, 9, 5], device='cuda:0') tensor([9, 3, 7, 9, 5], device='cuda:0')\n",
      "tensor([1, 4, 8, 8, 3], device='cuda:0') tensor([1, 4, 8, 8, 3], device='cuda:0')\n",
      "tensor([8, 4, 5, 9], device='cuda:0') tensor([8, 4, 5, 9], device='cuda:0')\n",
      "tensor([1, 2, 7], device='cuda:0') tensor([1, 2, 7], device='cuda:0')\n",
      "tensor([1, 1, 0, 5, 5], device='cuda:0') tensor([1, 1, 0, 5, 5], device='cuda:0')\n",
      "tensor([3, 4, 8, 4], device='cuda:0') tensor([3, 4, 8, 4], device='cuda:0')\n",
      "tensor([3, 6, 6], device='cuda:0') tensor([3, 6, 6], device='cuda:0')\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 1], device='cuda:0')\n",
      "tensor([1, 7, 2, 6, 3], device='cuda:0') tensor([1, 7, 2, 6, 3], device='cuda:0')\n",
      "tensor([3, 6, 3, 2], device='cuda:0') tensor([3, 6, 3, 2], device='cuda:0')\n",
      "tensor([1, 5, 8, 1], device='cuda:0') tensor([1, 5, 8, 1], device='cuda:0')\n",
      "tensor([1, 8, 0, 0], device='cuda:0') tensor([1, 8, 0, 0], device='cuda:0')\n",
      "tensor([6, 6, 4, 1], device='cuda:0') tensor([6, 6, 4, 1], device='cuda:0')\n",
      "tensor([1, 8, 2, 8, 1], device='cuda:0') tensor([1, 8, 2, 8, 1], device='cuda:0')\n",
      "tensor([1, 0, 8, 9], device='cuda:0') tensor([1, 0, 8, 9], device='cuda:0')\n",
      "tensor([1, 5, 8, 3, 5], device='cuda:0') tensor([1, 5, 8, 3, 5], device='cuda:0')\n",
      "tensor([3, 9, 6, 4], device='cuda:0') tensor([3, 9, 6, 4], device='cuda:0')\n",
      "tensor([9, 5, 8, 4, 0], device='cuda:0') tensor([9, 5, 8, 4, 0], device='cuda:0')\n",
      "tensor([1, 9], device='cuda:0') tensor([1, 9], device='cuda:0')\n",
      "tensor([1, 4, 0], device='cuda:0') tensor([1, 4, 0], device='cuda:0')\n",
      "tensor([8, 6, 4, 0, 8], device='cuda:0') tensor([8, 6, 4, 0, 8], device='cuda:0')\n",
      "tensor([5, 2, 4, 7, 7], device='cuda:0') tensor([5, 2, 4, 7, 7], device='cuda:0')\n",
      "tensor([1, 0, 3, 2, 5], device='cuda:0') tensor([1, 0, 3, 2, 5], device='cuda:0')\n",
      "tensor([1, 2, 8], device='cuda:0') tensor([1, 2, 8], device='cuda:0')\n",
      "tensor([1, 4, 8], device='cuda:0') tensor([1, 4, 8], device='cuda:0')\n",
      "tensor([1, 6, 9, 4], device='cuda:0') tensor([1, 6, 9, 4], device='cuda:0')\n",
      "tensor([8, 4], device='cuda:0') tensor([8, 4], device='cuda:0')\n",
      "tensor([2, 1, 6, 8, 4], device='cuda:0') tensor([2, 1, 6, 8, 4], device='cuda:0')\n",
      "tensor([3, 3, 0, 0], device='cuda:0') tensor([3, 3, 0, 0], device='cuda:0')\n",
      "tensor([7, 1, 7, 0], device='cuda:0') tensor([7, 1, 7, 0], device='cuda:0')\n",
      "tensor([1, 5], device='cuda:0') tensor([1, 5], device='cuda:0')\n",
      "tensor([1, 1, 8], device='cuda:0') tensor([1, 1, 8], device='cuda:0')\n",
      "tensor([7, 4, 9, 0, 0], device='cuda:0') tensor([7, 4, 9, 0, 0], device='cuda:0')\n",
      "tensor([5, 4, 3], device='cuda:0') tensor([5, 4, 3], device='cuda:0')\n",
      "tensor([1, 5, 7, 7, 7], device='cuda:0') tensor([1, 5, 7, 7, 7], device='cuda:0')\n",
      "tensor([4, 5], device='cuda:0') tensor([4, 5], device='cuda:0')\n",
      "tensor([2, 9, 9, 5, 9], device='cuda:0') tensor([1, 9, 9, 5, 9], device='cuda:0')\n",
      "tensor([7], device='cuda:0') tensor([7], device='cuda:0')\n",
      "tensor([2, 0, 5, 2], device='cuda:0') tensor([2, 0, 5, 2], device='cuda:0')\n",
      "tensor([1, 3, 7, 4, 5], device='cuda:0') tensor([1, 3, 7, 4, 5], device='cuda:0')\n",
      "tensor([5, 1, 7, 1, 4], device='cuda:0') tensor([5, 1, 7, 1, 4], device='cuda:0')\n",
      "tensor([2, 3, 9, 0, 5], device='cuda:0') tensor([2, 3, 9, 0, 5], device='cuda:0')\n",
      "tensor([5], device='cuda:0') tensor([5], device='cuda:0')\n",
      "tensor([5, 9, 2, 2, 7], device='cuda:0') tensor([5, 9, 2, 2, 7], device='cuda:0')\n",
      "tensor([1, 3, 9, 1, 6, 5], device='cuda:0') tensor([1, 3, 9, 7, 6, 5], device='cuda:0')\n",
      "tensor([9, 4, 6, 6, 7], device='cuda:0') tensor([9, 4, 6, 6, 7], device='cuda:0')\n",
      "tensor([3, 8, 0, 6, 3], device='cuda:0') tensor([3, 8, 0, 6, 3], device='cuda:0')\n",
      "tensor([2, 1, 4], device='cuda:0') tensor([2, 1, 4], device='cuda:0')\n",
      "tensor([2, 6, 4, 0, 5], device='cuda:0') tensor([2, 6, 4, 0, 5], device='cuda:0')\n",
      "tensor([3, 7, 2, 6], device='cuda:0') tensor([3, 7, 2, 6], device='cuda:0')\n",
      "tensor([3, 2, 9, 3], device='cuda:0') tensor([3, 1, 9, 3], device='cuda:0')\n",
      "tensor([4, 4, 4], device='cuda:0') tensor([4, 4, 4], device='cuda:0')\n",
      "tensor([2, 6], device='cuda:0') tensor([2, 6], device='cuda:0')\n",
      "tensor([7, 4, 5, 0], device='cuda:0') tensor([7, 2, 5, 0], device='cuda:0')\n",
      "tensor([2, 9, 3], device='cuda:0') tensor([2, 9, 3], device='cuda:0')\n",
      "tensor([4, 7, 6, 0, 1], device='cuda:0') tensor([4, 7, 6, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 2, 2], device='cuda:0') tensor([1, 1, 2, 2], device='cuda:0')\n",
      "tensor([4, 8, 3, 8], device='cuda:0') tensor([4, 8, 3, 8], device='cuda:0')\n",
      "tensor([6, 2, 6, 6], device='cuda:0') tensor([6, 2, 6, 6], device='cuda:0')\n",
      "tensor([3, 9], device='cuda:0') tensor([3, 9], device='cuda:0')\n",
      "tensor([3, 6], device='cuda:0') tensor([3, 6], device='cuda:0')\n",
      "tensor([1, 7, 6, 6, 8], device='cuda:0') tensor([1, 7, 6, 6, 8], device='cuda:0')\n",
      "tensor([6, 3, 8, 4, 5], device='cuda:0') tensor([6, 3, 8, 4, 5], device='cuda:0')\n",
      "tensor([6, 4, 0, 9], device='cuda:0') tensor([6, 4, 0, 9], device='cuda:0')\n",
      "tensor([8, 2, 7], device='cuda:0') tensor([8, 2, 7], device='cuda:0')\n",
      "tensor([5, 4, 6, 6, 7], device='cuda:0') tensor([5, 4, 6, 6, 7], device='cuda:0')\n",
      "tensor([1, 1, 5, 5], device='cuda:0') tensor([1, 1, 5, 5], device='cuda:0')\n",
      "tensor([2, 6, 6], device='cuda:0') tensor([2, 6, 6], device='cuda:0')\n",
      "tensor([1, 6, 3, 3], device='cuda:0') tensor([1, 6, 3, 3], device='cuda:0')\n",
      "tensor([1, 2, 6, 4], device='cuda:0') tensor([1, 2, 6, 4], device='cuda:0')\n",
      "tensor([2], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([1, 7, 9], device='cuda:0') tensor([1, 7, 9], device='cuda:0')\n",
      "tensor([1, 3, 5, 2, 0], device='cuda:0') tensor([1, 3, 5, 2, 0], device='cuda:0')\n",
      "tensor([7, 7, 1, 0], device='cuda:0') tensor([7, 7, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 7], device='cuda:0') tensor([1, 1, 7], device='cuda:0')\n",
      "tensor([6], device='cuda:0') tensor([6], device='cuda:0')\n",
      "tensor([1, 8, 8, 6, 2], device='cuda:0') tensor([1, 8, 8, 6, 2], device='cuda:0')\n",
      "tensor([2, 6, 1, 7], device='cuda:0') tensor([2, 6, 1, 7], device='cuda:0')\n",
      "tensor([9, 1, 6], device='cuda:0') tensor([9, 1, 6], device='cuda:0')\n",
      "tensor([2, 7], device='cuda:0') tensor([2, 7], device='cuda:0')\n",
      "tensor([3, 3, 6, 0, 1], device='cuda:0') tensor([3, 3, 6, 0, 1], device='cuda:0')\n",
      "tensor([5, 7], device='cuda:0') tensor([5, 7], device='cuda:0')\n",
      "tensor([5, 6, 4, 8, 5], device='cuda:0') tensor([5, 6, 4, 8, 5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ntot = 30\n",
    "\n",
    "acc = []\n",
    "\n",
    "stoi = {'0': 0, '1': 1, '2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,'+': 10,'=': 11, 'x': 12}\n",
    "itos = {v: k for k, v in stoi.items()}\n",
    "\n",
    "for j in range(50):\n",
    "\n",
    "    num_ex = 2\n",
    "    data_ff = data[j*num_ex:(j+1)*num_ex, :2*20 + 1].to('cuda:0')\n",
    "\n",
    "    u = 0\n",
    "    inputs = data_ff\n",
    "    target_ff = target[j*num_ex:(j+1)*num_ex].to('cuda:0')\n",
    "    max_gen = 0\n",
    "\n",
    "    while max_gen < ntot + 1:\n",
    "        seq_len = inputs.shape[-1]\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).to('cuda:0')\n",
    "\n",
    "        out = model(inputs, mask)[:, -1]\n",
    "        next_tok = torch.argmax(out, -1)\n",
    "\n",
    "        u = next_tok\n",
    "        max_gen += 1\n",
    "        inputs = torch.cat((inputs, next_tok.unsqueeze(1)), 1)\n",
    "\n",
    "        if (max_gen == ntot + 1):\n",
    "            accp = []\n",
    "            for i in range(inputs.shape[0]):\n",
    "                in_z = inputs[i, -max_gen:][(inputs[i, -max_gen:] != 12) & (inputs[i, -max_gen:] != 11)]\n",
    "                target_z = target_ff[i, -max_gen:][(target_ff[i, -max_gen:] != 12) & (target_ff[i, -max_gen:] != 11)]\n",
    "                print(in_z, target_z)\n",
    "                try:\n",
    "                    acc_z = (in_z == target_z).float().min().item()\n",
    "\n",
    "                    accp.append(acc_z)\n",
    "                except:\n",
    "                    continue\n",
    "            m = sum(accp) / len(accp) if len(accp) > 0 else 0.0\n",
    "            acc.append(m)\n",
    "\n",
    "            # s = inputs[:, -21:].reshape(-1)\n",
    "\n",
    "            # sp = s[s!=12]\n",
    "\n",
    "            # t = target_ff[:, -21:].reshape(-1)\n",
    "\n",
    "            # tp = t[t!=12]\n",
    "\n",
    "            # sz = ''.join([itos[q] for q in sp.tolist()]).split('=')[:-1]\n",
    "            # tz = ''.join([itos[q] for q in tp.tolist()]).split('=')[:-1]\n",
    "\n",
    "            # sz = torch.tensor([int(szz) for szz in sz])\n",
    "            # tz = torch.tensor([int(tzz) for tzz in tz])\n",
    "            \n",
    "            # length = min(sz.shape[0], tz.shape[0])\n",
    "\n",
    "            # sz = sz[:length]\n",
    "            # tz = tz[:length]\n",
    "\n",
    "            # acc.append((sz == tz).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acc) / len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 8, 7], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 8, 7], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxx191xxxx+xxxxx296xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx487xxx\n",
      "xxxxxxxxxxxx191xxxx+xxxxx296xxxxxxxxxxxxxxxxxxxxx487xxxxxxxxxxxxxxxxxx=\n"
     ]
    }
   ],
   "source": [
    "s0, s1 = [], []\n",
    "for ch0, ch1 in zip(inputs[1].tolist(), target_ff[1].tolist()):\n",
    "    s0.append(itos[ch0])\n",
    "    s1.append(itos[ch1])\n",
    "print(''.join(s0))\n",
    "print(''.join(s1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1, 1, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb = model.embed.Emb.weight.data.detach().cpu()\n",
    "\n",
    "svdEMB = torch.svd(emb)\n",
    "\n",
    "a, b = 0, 1\n",
    "\n",
    "x = svdEMB[0][:, a] * svdEMB[1][a]\n",
    "y = svdEMB[0][:, b] * svdEMB[1][b]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "i = 0\n",
    "for x, y in zip(x, y):\n",
    "    plt.annotate(f'{i}', (x, y))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = [''.join([itos[st_] for st_ in inputs[i, -21:].tolist()]) for i in range(len(inputs))]\n",
    "\n",
    "\n",
    "spq = [target_ffp[i] in [int(s) for s in ' '.join(sp[i].split('x')[:-1]).split()] for i in range(len(inputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(spq).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 12, 12, 12,  2,  2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 10, 12, 12, 12, 12, 12, 12,  1,  0, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12,  3,  2, 12, 11], device='cuda:1')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12, 12, 12,  2,  2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 10, 12, 12, 12, 12, 12, 12,  1,  0, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  3,  2,\n",
       "         12, 12, 12, 12, 12, 12, 11], device='cuda:1'),\n",
       " tensor([12, 12, 12, 12,  2,  2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 10, 12, 12, 12, 12, 12, 12,  1,  0, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12,  3,  2, 12, 11], device='cuda:1'),\n",
       " 32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ff[0], inputs[0], target_ffp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
